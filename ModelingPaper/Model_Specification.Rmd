---
title: "Model_Specification"
output: html_document
---

Does the model specification steps


For a given onset definition, perform model specification steps

```{r}

library(ggplot2)
library(tidyverse)
library(dplyr)
library(rgdal)
library(raster)
library(sf)
library(sp)
library(leaps)
library(viridis)
library(spatial)
library(spdep)
library(plm)
library(lmtest)
library(Metrics)
library(randomForest)

# select the onset definition
onset_type <- 'freq_10_persiann' #'AA_25_chirps_ATchirps5km' # 'Gabriel_onset'

# import data for all sections
os_system <- 'windows' # mac for laptop or windows for desktop
if (os_system == 'windows') {first_folder <- 'E:'}
if (os_system == 'mac') {first_folder <- '~/Documents'}
if (os_system == 'windows_laptop') {first_folder <- 'D:'}

#E:/R-code/Modeling/code/FCN_clean_csvs.R
#~/Documents/R-code
source(paste0(first_folder,'/R-code2/Modeling/code/FCN_clean_csvs.R'))
source(paste0(first_folder,'/R-code2/Modeling/code/FCN_plotting.R'))
source(paste0(first_folder,'/R-code2/Modeling/code/FCN_sample_data.R'))
source(paste0(first_folder,'/R-code2/Modeling/code/FCN_run_model_spatial_sampled.R'))

MT_outline <- readOGR(dsn = paste0(first_folder,'/R-code2/Modeling/data/shp/MatoGrossoOutline'), layer = 'MatoGrossoOutline')
crs(MT_outline) <- CRS("+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0")

min_soy_area <- 2 #km2. min area of total or SC/DC soy in cell, muni or property to be considered in model

```

Import and clean data

TO DO: AFTER EXTRACT CAR POLY AND MUNI SCALE DATA FOR THE NEW ONSET DEFINITIONS, WON'T NEED TO HAVE SECTION-SPECIFIC DATA IMPORTS AND CLEANING

Cell data

```{r}

# cell data -------------------------------------------------------------------------------------------
filename_cell_median <- paste0(first_folder,'/R-code-large-files/data_onset_', onset_type, 
                            '/median_onset_cell_', onset_type, '.csv')
filename_cell_percentile5 <- paste0(first_folder,'/R-code-large-files/data_onset_', onset_type, 
                                 '/percentile5_onset_cell_', onset_type, '.csv')
filename_cell_percentile25 <- paste0(first_folder,'/R-code-large-files/data_onset_', onset_type, 
                                  '/percentile25_onset_cell_', onset_type, '.csv')
                                     
filename_cell_shp<- paste0(first_folder,'/R-code-large-files/data_onset_', onset_type, '/shp')
layername_cell_shp <- paste0('median_onset_cell_SHP_', onset_type)

median_cell_raw <- read.csv(filename_cell_median)
percentile5_cell_raw <- read.csv(filename_cell_percentile5)
percentile25_cell_raw <- read.csv(filename_cell_percentile25)

cell_sf <- st_read(dsn = filename_cell_shp, layer = layername_cell_shp)

# clean data --------------------------------------------------------------------------------------------

# csv data 
median_cell <- median_cell_raw %>% delete_cols_median_cell() %>%
                                    rename_cols_median_cell()
median_cell$plant_stat_type <- rep('median', nrow(median_cell))
percentile5_cell <- percentile5_cell_raw %>% rename_cols_percentile_cell()
percentile25_cell <- percentile25_cell_raw %>% rename_cols_percentile_cell()

# sf data 

# add cell_ID
clean_cell_ID <- function(cell_ID) {
  strsplit(cell_ID, "_")[[1]][2]
}

# get cell_ID column for median
cell_sf$cell_ID <- median_cell$cell_ID
cell_sf$cell_ID <- sapply(as.character(cell_sf$cell_ID), clean_cell_ID)

# join median, percentile data to cell_sf
# cell_sf has median information, but copy it and put in percentile info for DC and SC plant
cell_sf$plant_stat_type <- rep("median", nrow(cell_sf))

cell_sf_percentile5 <- cell_sf
cell_sf_percentile5$SC_plant <- percentile5_cell$SC_plant
cell_sf_percentile5$DC_plant <- percentile5_cell$DC_plant
cell_sf_percentile5$plant_stat_type <- rep("percentile5", nrow(cell_sf_percentile5))

cell_sf_percentile25 <- cell_sf
cell_sf_percentile25$SC_plant <- percentile25_cell$SC_plant
cell_sf_percentile25$DC_plant <- percentile25_cell$DC_plant
cell_sf_percentile25$plant_stat_type <- rep("percentile25", nrow(cell_sf_percentile25))

cell_sf <- rbind(cell_sf, cell_sf_percentile5, cell_sf_percentile25)

cell_sf_tidy <- cell_sf %>% tidy_by_intensity_plant("SC_plant", "DC_plant") %>%
            dplyr::select(-c(SC_harvest, DC_harvest)) %>%
            categorize_regions_cell_sf_tidy() # categorize cells into four regions

cell_sf_tidy$year_index <- cell_sf_tidy$year - 2003
cell_sf_tidy$year_factor <- as.factor(cell_sf_tidy$year)

cell_sf_tidy <- cell_sf_tidy %>%  drop_na
cell_sf_tidy$delay <- cell_sf_tidy$plant - cell_sf_tidy$onset 
```

CAR poly data

```{r}

# import data
filename_CARpoly_median <- paste0(first_folder,'/R-code-large-files/data_onset_', onset_type, 
                            '/median_CARpoly_', onset_type, '.csv')
filename_CARpoly_percentile5 <- paste0(first_folder,'/R-code-large-files/data_onset_', onset_type, 
                                 '/percentile5_CARpoly_', onset_type, '.csv')
filename_CARpoly_percentile25 <- paste0(first_folder,'/R-code-large-files/data_onset_', onset_type, 
                                  '/percentile25_CARpoly_', onset_type, '.csv')

filename_CARpoly_shp<- paste0(first_folder,'/R-code-large-files/data_onset_', onset_type, '/shp')
layername_cell_shp <- paste0('median_CARpoly_SHP_', onset_type)

median_CARpoly_raw <- read.csv(filename_CARpoly_median)
percentile5_CARpoly_raw <- read.csv(filename_CARpoly_percentile5)
percentile25_CARpoly_raw <- read.csv(filename_CARpoly_percentile25)

CARpoly_sf <- st_read(dsn = filename_CARpoly_shp, layer = layername_CARpoly_shp)

# clean data -----------------
median_CARpoly <- median_CARpoly_raw %>% delete_cols_median_CARpoly() %>%
                                    rename_cols_median_CARpoly() #rename_cols_median_CARpoly(median_CARpoly_raw)
percentile5_CARpoly <- percentile5_CARpoly_raw %>% rename_cols_percentile5_CARpoly() #rename_cols_percentile5_CARpoly(percentile5_CARpoly_raw)
percentile95_CARpoly <- percentile25_CARpoly_raw %>% rename_cols_percentile25_CARpoly() #rename_cols_percentile95_CARpoly(percentile95_CARpoly_raw)
#CARpoly_raw <- create_CARpoly_raw(median_CARpoly_raw, percentile5_CARpoly_raw, percentile95_CARpoly_raw)

# CARpoly_tidy <- tidy_CARpoly(CARpoly_raw)
# 
# CARpoly_tidy <- categorize_vars_CARpoly_tidy(CARpoly_tidy) %>% delete_cols_CARpoly_tidy()
# CARpoly_untidy <- categorize_vars_CARpoly_untidy(CARpoly_raw) 
# # change rename cols and delete unnecessary cols
# CARpoly_untidy <- CARpoly_untidy %>% rename_cols_CARpoly_untidy() %>%
#                                     delete_cols_CARpoly_untidy()

CARpoly_sf$plant_stat_type <- rep("median", nrow(CARpoly_sf))

CARpoly_sf_percentile5 <- CARpoly_sf
CARpoly_sf_percentile5$SC_plant <- percentile5_CARpoly$SC_plant
CARpoly_sf_percentile5$DC_plant <- percentile5_CARpoly$DC_plant
CARpoly_sf_percentile5$plant_stat_type <- rep("percentile5", nrow(CARpoly_sf_percentile5))

CARpoly_sf_percentile25 <- CARpoly_sf
CARpoly_sf_percentile25$SC_plant <- percentile25_CARpoly$SC_plant
CARpoly_sf_percentile25$DC_plant <- percentile25_CARpoly$DC_plant
CARpoly_sf_percentile25$plant_stat_type <- rep("percentile25", nrow(CARpoly_sf_percentile25))

CARpoly_sf <- rbind(CARpoly_sf, CARpoly_sf_percentile5, CARpoly_sf_percentile25)

CARpoly_sf_tidy <- CARpoly_sf %>% tidy_by_intensity_plant("SC_plant", "DC_plant") %>%
            dplyr::select(-c(SC_harvest, DC_harvest)) %>%
            categorize_regions_CARpoly_sf_tidy() # categorize cells into four regions

CARpoly_sf_tidy$year_index <- CARpoly_sf_tidy$year - 2003
CARpoly_sf_tidy$year_factor <- as.factor(CARpoly_sf_tidy$year)

CARpoly_sf_tidy <- CARpoly_sf_tidy %>%  drop_na
CARpoly_sf_tidy$delay <- CARpoly_sf_tidy$plant - CARpoly_sf_tidy$onset 

```

```{r}

# import data -----------------------------------------------------------------------------------------------

filename_muni_median <- paste0(first_folder,'/R-code-large-files/data_onset_', onset_type, 
                            '/median_muni_', onset_type, '.csv')
filename_muni_percentile5 <- paste0(first_folder,'/R-code-large-files/data_onset_', onset_type, 
                                 '/percentile5_muni_', onset_type, '.csv')
filename_muni_percentile25 <- paste0(first_folder,'/R-code-large-files/data_onset_', onset_type, 
                                  '/percentile25_muni_', onset_type, '.csv')

filename_muni_shp<- paste0(first_folder,'/R-code-large-files/data_onset_', onset_type, '/shp')
layername_muni_shp <- paste0('median_muni_SHP_', onset_type)

median_muni_raw <- read.csv(filename_muni_median)
percentile5_muni_raw <- read.csv(filename_muni_percentile5)
percentile25_muni_raw <- read.csv(filename_muni_percentile25)

muni_sf <- st_read(dsn = filename_muni_shp, layer = layername_muni_shp)


# clean data ------------------------------------------------------------------------------

# median muni
median_muni <- median_muni_raw %>% rename_cols_median_muni()
percentile5_muni <- percentile5_muni_raw %>% filter(year > 0) %>% filter(Muni_code > 0)
percentile25_muni <- percentile25_muni_raw %>% filter(year > 0)  %>% filter(Muni_code > 0)

# create tidy datasets

muni_tidy <- tidy_combine_muni(median_muni, percentile5_muni, percentile25_muni)

muni_tidy <- categorize_vars_muni_tidy(muni_tidy)
muni_untidy <- categorize_vars_muni_untidy(median_muni)


muni_sf$plant_stat_type <- rep("median", nrow(muni_sf))

# get rid of rows in muni_sf with NA in planting dates and Muni_code
muni_sf <- muni_sf[!is.na(muni_sf$SC_plant) | !is.na(muni_sf$DC_plant),]

muni_sf_percentile5 <- muni_sf
muni_sf_percentile5$SC_plant <- percentile5_muni$single_plant
muni_sf_percentile5$DC_plant <- percentile5_muni$double_plant
muni_sf_percentile5$plant_stat_type <- rep("percentile5", nrow(muni_sf_percentile5))

muni_sf_percentile25 <- muni_sf
muni_sf_percentile25$SC_plant <- percentile25_muni$single_plant
muni_sf_percentile25$DC_plant <- percentile25_muni$double_plant
muni_sf_percentile25$plant_stat_type <- rep("percentile25", nrow(muni_sf_percentile25))

muni_sf <- rbind(muni_sf, muni_sf_percentile5, muni_sf_percentile25)

muni_sf_tidy <- muni_sf %>% tidy_by_intensity_plant("SC_plant", "DC_plant") %>%
            dplyr::select(-c(SC_harvest, DC_harvest)) 

muni_sf_tidy$year_index <- muni_sf_tidy$year - 2003
muni_sf_tidy$year_factor <- as.factor(muni_sf_tidy$year)

muni_sf_tidy <- muni_sf_tidy %>%  drop_na
muni_sf_tidy$delay <- muni_sf_tidy$plant - muni_sf_tidy$onset 
```


Choose observation scale

TO DO
NOTE, THIS IS FOR GABRIEL'S AA ONSET BECAUSE DON'T HAVE MUNI AND CARPOLY EXTRACTIONS FOR OTHER ONSET DEFINITIONS YET
RUN FOR EACH PLANTING PERCENTILE (5, 25, 50; HERE, DON'T HAVE 25 BUT HAVE 95)

```{r}



# Criteria 1: R2 of OLS model at each observation scale ----------------------------------------------------------

# Run OLS model with (1) all data points, (2) year, onset, lat, lon, for (3) the chosen onset, (4) at each observation scale (cell 25km, CARpoly, muni) and (5) each planting percentile
# Return R2 for each observation scale

model_CARpoly_median <- lm(plant ~ onset + year + lat + lon + intensity + plant_stat_type, data = CARpoly_sf_tidy)
model_CARpoly_percentile5 <- lm(plant ~ onset + year + lat + lon + intensity + plant_stat_type, data = CARpoly_sf_tidy)
model_CARpoly_percentile95 <- lm(plant ~ onset + year + lat + lon + intensity + plant_stat_type, data = CARpoly_sf_tidy)

model_cell_median <- lm(plant ~ onset + year + lat + lon + intensity + plant_stat_type, data = cell_sf_tidy)
model_cell_percentile5 <- lm(plant ~ onset + year + lat + lon + intensity + plant_stat_type, data = cell_sf_tidy)
model_cell_percentile95 <- lm(plant ~ onset + year + lat + lon + intensity + plant_stat_type, data = cell_sf_tidy)

model_muni_median <- lm(plant ~ onset + year + lat + lon + intensity + plant_stat_type, data = muni_sf_tidy)
model_muni_percentile5 <- lm(plant ~ onset + year + lat + lon + intensity + plant_stat_type, data = muni_sf_tidy)
model_muni_percentile95 <- lm(plant ~ onset + year + lat + lon + intensity + plant_stat_type, data = muni_sf_tidy)

R2_CARpoly_median <- summary(model_CARpoly_median)$r.squared
R2_CARpoly_percentile5 <- summary(model_CARpoly_percentile5)$r.squared
R2_CARpoly_percentile95 <- summary(model_CARpoly_percentile95)$r.squared

R2_cell_median <- summary(model_cell_median)$r.squared
R2_cell_percentile5 <- summary(model_cell_percentile5)$r.squared
R2_cell_percentile95 <- summary(model_cell_percentile95)$r.squared

R2_muni_median <- summary(model_muni_median)$r.squared
R2_muni_percentile5 <- summary(model_muni_percentile5)$r.squared
R2_muni_percentile95 <- summary(model_muni_percentile95)$r.squared

print(paste('R2 for median (CARpoly, cell, and muni):', R2_CARpoly_median, R2_cell_median, R2_muni_median))
print(paste('R2 for percentile5 (CARpoly, cell, and muni):', R2_CARpoly_percentile5, R2_cell_percentile5, R2_muni_percentile5))
print(paste('R2 for percentile95 (CARpoly, cell, and muni):', R2_CARpoly_percentile95, R2_cell_percentile95, R2_muni_percentile95))

# Criteria 2: variability of planting date
for (intensity in c('SC', 'DC')) {
  for (percentile in c('plant_median', 'plant_percentile5', 'plant_percentile95')) {
    sd_CARpoly = sd(CARpoly_sf_tidy[CARpoly_sf_tidy$intensity == intensity, percentile], na.rm = TRUE)
    sd_cell = sd(cell_sf_tidy[cell_sf_tidy$intensity == intensity, percentile], na.rm = TRUE)
    sd_muni = sd(muni_sf_tidy[muni_sf_tidy$intensity == intensity, percentile], na.rm = TRUE)
    
    print('--------------------')
    print(percentile)
    print(intensity)
    print(paste('CAR poly', intensity, percentile, 'planting date std: ', sd_CARpoly))
    print(paste('cell', intensity, percentile, 'planting date std: ', sd_cell))
    print(paste('muni', intensity, percentile, 'planting date std: ', sd_muni))
  }
}

# Criteria 3: average size of the observation scales

```

Plot median planting date, aggregated to cell, carpoly and muni scales

TO DO
NOTE, DO MUNI AND CARPOLY EXTRACTIONS AND THEN MAP. MAYBE ZOOM IN ON A REGION FOR CARPOLY?

```{r}

year_oi <- 2012 # to map
intensity_oi <- "DC" # to map


# filter a specific year
cell_sf_tidy_year <- cell_sf_tidy %>% filter(year == year_oi) %>%
                                   filter(intensity == intensity_oi)
muni_sf_tidy_year <- muni_sf_tidy %>% filter(year == year_oi) %>%
                                   filter(intensity == intensity_oi)

ggplot(cell_sf_tidy_year) +
  geom_sf(aes(fill = plant)) + # , colour = NA) +
  scale_fill_viridis() +
  ggtitle(paste("Cell median planting date", year_oi, intensity_oi)) +
  geom_polygon(data = MT_outline, aes(x = long, y = lat), color = "black", alpha = 0, linetype = 1) +
  theme_bw()

ggplot(muni_sf_tidy_year) +
  geom_sf(aes(fill = plant)) + #, colour = NA) +
  scale_fill_viridis() +
  ggtitle(paste("Muni median planting date", year_oi, intensity_oi)) +
  geom_polygon(data = MT_outline, aes(x = long, y = lat), color = "black", alpha = 0, linetype = 1) +
  theme_bw()
```

Choose predictors, using all data points and only cell scale and the chosen onset definition

TO DO 
EXTRACT ONSET_HISTORICALRANGE AND TEST

```{r}

# Only OLS prediction, using all data points -------------------------------------------------------------------

# Choose from the following pool: onset, year, intensity, lat, lon, age of soy, region, range of historical onset from 2004 to 2014, onset:intensity, percentile, onset:percentile, onset:year, onset:lat, onset:lon and maybe other interactions

# Method 1: build a semi-constrained stepwise regression based on AIC
base_model <- lm(plant ~ onset, data = cell_sf_tidy)
selected_model_AIC <- step(base_model, scope=list(upper = ~ 0+onset + year_index + intensity + lat + lon +  region +
                                                plant_stat_type + onset:intensity + onset:plant_stat_type +
                                                onset:lat + onset:lon, lower = ~ 0+onset), direction = 'forward', trace = FALSE)
print('selected OLS model using AIC')
summary(selected_model_AIC)

# Method 2: add stepwise according to adj R2 and partial F test

do_lm <- function(data, y.var,  x.vars, mask.soy.area) {

  # x.vars = vector of predictors, names of variables are in quotes, and interactions as will be written in the lm formula, e.g. onset:latitude
  # mask.soy.area = TRUE or FALSE, if TRUE take out rows with total soy area below min_soy_area
  
  # get rid of observations with low soy area
  data.subset <- if(mask.soy.area) {
    data[data$soy_area_k >= min_soy_area,]
  }
  
  # define the formula --------------------------------------------------------------------------------------
  # define the basic formula
  formula.string <- paste(y.var, paste(x.vars, collapse = " + "), sep = " ~ ")
  
  f <- as.formula(formula.string)

  # do the model ----------------------------------------------------------------------------------------
  # evaluate model. note, simpler alternative: model <- lm(f, data = data.subset)
  model <- eval(bquote(   lm(.(f), data = data.subset)   ))
  
  return(model)
}

# given an initial model, step through potential new predictors and pick the best one. 
add_predictor_stepwise <- function(data, initial.predictors, y.var, new.predictors, mask.soy.area) {
  
  # given an initial set of predictors, add another one based on adj R2
  # interactions to explore are in new.predictors
  
  # calculate adj R2 of initial model
  initial_model <- do_lm(data = data, 
              y.var = y.var, 
              x.vars = initial.predictors,
              mask.soy.area = mask.soy.area)
  
  initial_adjR2 <- summary(initial_model)$adj.r.squared
  
  # initialize 'best' new predictor and best adj_R2
  best_new_predictor <- "none"
  current_best_adjR2 <- initial_adjR2
  
  # initialize vector to store adj R2
  new_adjR2s <- c(initial_adjR2)
  
  # loop through potential new predictors
  for (predictor in new.predictors) {
    
    # calculate new adjR2 and save it
    new_model <- do_lm(data = data, 
              y.var = y.var, 
              x.vars = c(initial.predictors, predictor), 
              mask.soy.area = mask.soy.area)
    
    new_adjR2 <- summary(new_model)$adj.r.squared
    new_adjR2s <- c(new_adjR2s, new_adjR2)
    
    # if new_adjR2 is better, update the new_best_predictor
    if (new_adjR2 > current_best_adjR2) {
      current_best_adjR2 <- new_adjR2
      best_new_predictor <- predictor
    }
  }
  
  # return best predictor, its adjR2, and the list of other predictors' R2
  all_adjR2 <- data.frame(model = c("initial", new.predictors), adjR2 = new_adjR2s)
  
  return(list(new_predictor = best_new_predictor, 
              new_adjR2 = current_best_adjR2,  
              adjR2_table = all_adjR2))
}

# runs add_predictor_stepwise to build the final model
produce_model_stepwise <- function(data, initial.predictors, y.var, new.predictors,  mask.soy.area, max.predictors) {
  # notes
  # max.predictors = the max allowed number of predictors in the model
  
  model_finalized <- FALSE
  iterations <- 0
  
  while (!model_finalized & iterations <= 10) {
    
    iterations <- iterations + 1
    
    # add a new predictor, save results
    result <- add_predictor_stepwise(data, initial.predictors, y.var, 
                       new.predictors, mask.soy.area)
    
    new_predictor <- result$new_predictor
    new_adjR2_table <- result$adjR2_table
    new_adjR2 <- result$new_adjR2

    #print(new_adjR2_table)
    
    # check if the model returned 'none' or the max.predictors was reached; if so, model is finalized
    # if model is finalized, return the model
    if (new_predictor == "none" | length(initial.predictors) >= max.predictors) { 
      model_finalized <- TRUE
      
      final_predictors <- initial.predictors
      final_adjR2 <- new_adjR2
      
      # run the final model to return the lm output
      final_model <- do_lm(data = data, 
              y.var = y.var, 
              x.vars = final_predictors, 
              mask.soy.area = mask.soy.area)
      
    }
    
    # if model isn't done adding predictors, update initial.predictors for the new iteration
    initial.predictors <- c(initial.predictors, new_predictor)
    
  }
  
  # return
  return(list(final_model = final_model,
              final_predictors = final_predictors,
              final_adjR2 = final_adjR2))
}


new.predictors <- c('year_index', 'intensity', 'lat', 'lon', 'region', 'plant_stat_type', 'onset:intensity', 'onset:plant_stat_type', 'onset:lat', 'onset:lon') 

selected_model_adjR2 <- produce_model_stepwise(data = cell_sf_tidy, initial.predictors = c('onset'), y.var = 'plant',
                       new.predictors = new.predictors, mask.soy.area = TRUE, max.predictors = 6)

# Return the top six predictors (and their p-values)
print('top six predictors selected by adj R2')
print(selected_model_adjR2$final_predictors)

# run model with the top six predictors (onset, percentile, intensity, year, region, lat) along with the interaction variables
# to compare the interaction variables with the different onset coefs when percentiles and intensities are fitted separately
selected_OLS_model <- lm(plant ~ onset + plant_stat_type + intensity + year_index + region + lat + onset:plant_stat_type + onset:intensity, data = cell_sf_tidy)

print('OLS with selected predictors + interesting interaction variables')
print(summary(selected_OLS_model))
```


Spatial and temporal autocorrelation (separate model for each intensity and percentile) - for all data

```{r}

# for both OLS and FE, run model with chosen predictors and all data points (separated by intensity and percentile)

# initialize storage of 
spatial_autocorr_alldata <- data.frame(model = character(0), year = numeric(0), p_value = numeric(0), 
                                       moran_i = numeric(0), intensity = character(0), percentile = character(0))

temp_autocorr_alldata <- data.frame(model = character(0), p_value = numeric(0), 
                                       dw = numeric(0), intensity = character(0), percentile = character(0),
                                    percent_autocorr = numeric(0))

# OLS
for (intensity in c('SC', 'DC')) {
  for (percentile in c('median', 'percentile5', 'percentile25')) {

    # OLS
    data_subset = cell_sf_tidy[(cell_sf_tidy$intensity == intensity) & (cell_sf_tidy$plant_stat_type == percentile), ]
    OLS_model = lm(plant ~ onset + year_index + region + lat, data = data_subset)

    data_subset$residuals <- residuals(OLS_model)

    # temporal autocorrelation
    data_subset_df <- data_subset

    # filter out all cells where there isn't data for all years
    st_geometry(data_subset_df) <- NULL
    data_subset_df <- data_subset_df[complete.cases(data_subset_df),]
    cells_list <- list()
    i <- 1
    for (year in 2004:2014) {
       cells_in_year <- data_subset_df[data_subset_df$year == year,]
       cells_list[[i]] <- cells_in_year$label
       i <- i + 1
    }
    full_data_cells <- Reduce(intersect, cells_list)
    data_subset_df<- data_subset_df[data_subset_df$label %in% full_data_cells, ]

    data_subset_df_nested <- group_by(data.frame(data_subset_df), label) %>% nest()

    dwtest_one_cell <- function(data) {
      dwtest(residuals ~ 1, data = data)
     }

    data_subset_df <- data_subset_df_nested %>%
       mutate(dwtest = map(data, dwtest_one_cell)) %>%
       mutate(test_df = map(dwtest, generics::tidy)) %>%
       unnest(test_df)

    # calculate proportion of p values below 5% significance, with Bonferroni correction

    percent_tempauto <- mean(data_subset_df$p.value < 0.05/nrow(data_subset_df))

    temp_autocorr_alldata <- rbind(temp_autocorr_alldata,
                                   data.frame(model = "OLS", intensity = intensity,
                                              percentile = percentile, dw = mean(data_subset_df$statistic),
                                              p_value = mean(data_subset_df$p.value),
                                              percent_autocorr = percent_tempauto))

    # spatial autocorrelation
    for (year_oi in 2004:2014) {
      # need 'one layer': one year, one intensity, set up weights
      to_autocorrelation <- data_subset[data_subset$year == year_oi, ]
      to_autocorrelation_sp <- as(to_autocorrelation, "Spatial")
      st_geometry(to_autocorrelation) <- NULL # turn to_autocorrelation into data frame
      centroids <- coordinates(to_autocorrelation_sp)
      to_autocorrelation_points <- SpatialPointsDataFrame(coords = centroids, data = to_autocorrelation)
      nb<-knn2nb(knearneigh(to_autocorrelation_points))
      lw <- nb2listw(nb, zero.policy = TRUE)

      # OLS: calculate spatial autocorrelation
      moran_residual <- moran.mc(to_autocorrelation_points$residuals, lw, 500, zero.policy = TRUE)
      residual_moran <- moran_residual$statistic
      residual_moran_pval <- moran_residual$p.value

      # save spatial autocorrelation information
      spatial_autocorr_alldata <- rbind(spatial_autocorr_alldata,
                                        data.frame(model = "OLS", year = year_oi,
                                                   p_value = residual_moran_pval, moran_i = residual_moran,
                                                   intensity = intensity, percentile = percentile))
    }

  }
}


# FE
for (intensity in c('SC', 'DC')) {
  for (percentile in c('median', 'percentile5', 'percentile25')) {
    
    data_subset = cell_sf_tidy[(cell_sf_tidy$intensity == intensity) & (cell_sf_tidy$plant_stat_type == percentile), ]
    data_subset_panel <- pdata.frame(data_subset,index = c("cell_ID"))
    FE_model <- plm(plant ~ onset + year_index, data =  data_subset_panel, model = "within")
    
    data_subset$residuals <- residuals(FE_model)
    
    # temporal autocorrelation
    data_subset_df <- data_subset

    # filter out all cells where there isn't data for all years
    st_geometry(data_subset_df) <- NULL
    data_subset_df <- data_subset_df[complete.cases(data_subset_df),]
    cells_list <- list()
    i <- 1
    for (year in 2004:2014) {
       cells_in_year <- data_subset_df[data_subset_df$year == year,]
       cells_list[[i]] <- cells_in_year$label
       i <- i + 1
    }
    full_data_cells <- Reduce(intersect, cells_list)
    data_subset_df<- data_subset_df[data_subset_df$label %in% full_data_cells, ]
     
    data_subset_df_nested <- group_by(data.frame(data_subset_df), label) %>% nest()
    
    dwtest_one_cell <- function(data) {
      dwtest(residuals ~ 1, data = data)
     }
     
    data_subset_df <- data_subset_df_nested %>%
       mutate(dwtest = map(data, dwtest_one_cell)) %>%
       mutate(test_df = map(dwtest, generics::tidy)) %>%
       unnest(test_df)
     
    # calculate proportion of p values below 5% significance, with Bonferroni correction
    
    percent_tempauto <- mean(data_subset_df$p.value < 0.05/nrow(data_subset_df))
    
    temp_autocorr_alldata <- rbind(temp_autocorr_alldata, 
                                   data.frame(model = "FE", intensity = intensity, 
                                              percentile = percentile, dw = mean(data_subset_df$statistic), 
                                              p_value = mean(data_subset_df$p.value), 
                                              percent_autocorr = percent_tempauto))

    # spatial autocorrelation
    for (year_oi in 2004:2014) {
      # need 'one layer': one year, one intensity, set up weights
      to_autocorrelation <- data_subset[data_subset$year == year_oi, ]
      to_autocorrelation_sp <- as(to_autocorrelation, "Spatial")
      st_geometry(to_autocorrelation) <- NULL # turn to_autocorrelation into data frame
      centroids <- coordinates(to_autocorrelation_sp)
      to_autocorrelation_points <- SpatialPointsDataFrame(coords = centroids, data = to_autocorrelation)
      nb<-knn2nb(knearneigh(to_autocorrelation_points)) 
      lw <- nb2listw(nb, zero.policy = TRUE)
      
      # calculate spatial autocorrelation
      moran_residual <- moran.mc(to_autocorrelation_points$residuals, lw, 500, zero.policy = TRUE)
      
      residual_moran <- moran_residual$statistic
      residual_moran_pval <- moran_residual$p.value
      
      # save spatial autocorrelation information
      spatial_autocorr_alldata <- rbind(spatial_autocorr_alldata, 
                                        data.frame(model = "FE", year = year_oi, 
                                                   p_value = residual_moran_pval, moran_i = residual_moran,
                                                   intensity = intensity, percentile = percentile))
    }
    
  }
}

# summarize spatial correlation when all data points are retained
morani_summary_alldata <- spatial_autocorr_alldata %>% 
  group_by(model, intensity, percentile) %>%
  dplyr::summarize(mean_moran_i = mean(moran_i), sd_moran_i = sd(moran_i))

moran_pval_summary_alldata <- spatial_autocorr_alldata %>% 
  group_by(model, intensity, percentile) %>%
  dplyr::summarize(mean_pval = mean(p_value), sd_pval = sd(p_value))

print('spatial autocorrelation when using all data')
print(moran_pval_summary_alldata)

print('temporal autocorrelation when using all data')
print(temp_autocorr_alldata)

```

Spatial sampling for autocorrelation for OLS

```{r}
# sampling grid: size choice for OLS (no spatial sampling needed for FE)
# for each sampling grid size (and offset), report spatial autocorrelation, prediction accuracy in new years and locations, percent total data used, coefficients' sensitivity to sampling grid position

# test different grid sizes and aggregation strategies. don't do any offset

do_elim_year = TRUE # TOGGLE TRUE IF USE YEAR AS TREND, FALSE IF USE YEAR AS FIXED EFFECT

results <- data.frame()

for (grid_size in seq(0.75, 1, 0.25)) { # 0.25 to 1.75, seq(0.75, 1.25, 0.25)
    for (lat_offset in seq(0, 1.25, by = 0.25)) { # 0 to 1.5
      for (lon_offset in seq(0, 1.25, by = 0.25)) { # 0 to 1.5
        
        data_subset = cell_sf_tidy[(cell_sf_tidy$intensity == intensity) & (cell_sf_tidy$plant_stat_type == percentile), ]
        
        model_output <- run_OLS(full_data = data_subset, 
                              predictors = c("onset", "lat", "year_index", "lon"), 
                          y.var = "plant", plant_stat = percentile,
                          grid_size = grid_size, 
                          lat_offset = lat_offset, lon_offset = lon_offset, agg_scheme = FALSE, 
                          plot_samples = FALSE, plot_model_evals = FALSE, 
                          year_oi = 2007,
                          do_elim_year = do_elim_year,
                          chosen_intensity = "DC")
        
        print(paste("grid size", grid_size))
       
        selected_output <- as.numeric(c(grid_size, lat_offset, lon_offset,
                             model_output$onset_coef, 
                             model_output$R2, model_output$percent_data_used, 
                             model_output$residual_moran_pval, model_output$onset_moran_pval, 
                             model_output$plant_moran_pval,
                             mean(model_output$prediction_results_elimlocation$RMSE, na.rm = TRUE)
                             ))
        
        if (do_elim_year) {
          selected_output <- c(selected_output, mean(model_output$prediction_results_elimyear$RMSE, na.rm = TRUE))
        }
        
        print(selected_output)
        results <- rbind(results, selected_output)
      }
    }
}

if (do_elim_year) {
  names(results) <- c("grid_size",  "lat_offset", "lon_offset", "onset_coef", 
                    "R2", "percent_used", "residual_moran_pval", "onset_moran_pval",
                    "plant_moran_pval",
                    "mean_RMSE_elimlocation", "mean_RMSE_elimyear")
}
if (!do_elim_year) {
  names(results) <- c("grid_size",  "lat_offset", "lon_offset", "onset_coef", 
                    "R2", "percent_used", "residual_moran_pval", "onset_moran_pval",
                    "plant_moran_pval",
                    "mean_RMSE_elimlocation")
}

# SAVED RESULTS
saveRDS(results, paste0("OLS_spatial_sampling_results_", onset_type, ".rds"))

# calculate mean and sd for each grid size x agg scheme, for different grid offsets
results_summary <- results %>%
                        group_by(grid_size) %>%
                        summarize(
                          onset_coef_mean = mean(onset_coef),
                          onset_coef_sd = sd(onset_coef),
                          R2_mean = mean(R2), R2_sd = sd(R2),
                          percent_used_mean = mean(percent_used),
                          percent_used_sd = sd(percent_used),
                          residual_moran_pval_mean = mean(residual_moran_pval),
                          residual_moran_pval_sd = sd(residual_moran_pval),
                          onset_moran_pval_mean = mean(onset_moran_pval),
                          onset_moran_pval_sd = sd(onset_moran_pval),
                          plant_moran_pval_mean = mean(plant_moran_pval),
                          plant_moran_pval_sd = sd(plant_moran_pval),
                          #RMSE_elimyear_mean = mean(mean_RMSE_elimyear), # TURN OFF IF USE YEAR AS FACTOR
                          #RMSE_elimyear_sd = sd(mean_RMSE_elimyear), # TURN OFF IF USE YEAR AS FACTOR
                          RMSE_elimlocation_mean = mean(mean_RMSE_elimlocation),
                          RMSE_elimlocation_sd = sd(mean_RMSE_elimlocation)
                        )

if (do_elim_year) {
  elim_year_summary <- results %>%
    group_by(grid_size) %>%
    summarize(
      RMSE_elimyear_mean = mean(mean_RMSE_elimyear),
      RMSE_elimyear_sd = sd(mean_RMSE_elimyear) 
    )
  
  results_summary <- cbind(results_summary, elim_year_summary)
}

```

Figures for choosing sampling grid size (grid size on x axis, and various metrics on y axis; with error bars for sampling grid location)

```{r}

colnames(results_summary) <- make.unique(names(results_summary))

# compare grid sizes
plot_results_summary <- function(data, x_data, y_data, y_sd, x_label, y_label) {
  ggplot(data, mapping = aes(x = x_data, y = y_data)) +
    geom_line(data, mapping = aes(x = x_data, y = y_data)) +
    geom_errorbar(aes(ymin=y_data-y_sd, ymax=y_data+y_sd), position=position_dodge(0.05)) +
    ggtitle(y_label) +
    xlab(x_label) +
    ylab(y_label)+
    theme_bw()
}

# plot results, summarizing for different grid offsets
plot_results_summary(data = results_summary, 
                     x_data = results_summary$grid_size, y_data = results_summary$onset_coef_mean,
                     y_sd = results_summary$onset_coef_sd,
                    x_label = "grid_size", y_label = "onset coefficient")

plot_results_summary(data = results_summary, 
                     x_data = results_summary$grid_size, y_data = results_summary$R2_mean,
                     y_sd = results_summary$R2_sd,
                    x_label = "grid_size", y_label = "R2")

plot_results_summary(data = results_summary, 
                     x_data = results_summary$grid_size, y_data = results_summary$percent_used_mean,
                     y_sd = results_summary$percent_used_sd,
                    x_label = "grid_size", y_label = "percent used")

plot_results_summary(data = results_summary, 
                     x_data = results_summary$grid_size, y_data = results_summary$residual_moran_pval_mean,
                     y_sd = results_summary$residual_moran_pval_sd,
                    x_label = "grid_size", y_label = "residual moran's I p value")

plot_results_summary(data = results_summary, 
                     x_data = results_summary$grid_size, y_data = results_summary$onset_moran_pval_mean,
                     y_sd = results_summary$onset_moran_pval_sd,
                    x_label = "grid_size", y_label = "onset moran's I p value")

plot_results_summary(data = results_summary, 
                     x_data = results_summary$grid_size, y_data = results_summary$plant_moran_pval_mean,
                     y_sd = results_summary$plant_moran_pval_sd,
                    x_label = "grid_size", y_label = "plant moran's I p value")

plot_results_summary(data = results_summary, 
                     x_data = results_summary$grid_size, y_data = results_summary$RMSE_elimlocation_mean,
                     y_sd = results_summary$RMSE_elimlocation_sd,
                    x_label = "grid_size", y_label = "RMSE from eliminating location")

if (do_elim_year) {
  plot_results_summary(data = results_summary, 
                       x_data = results_summary$grid_size, y_data = results_summary$RMSE_elimyear_mean,
                       y_sd = results_summary$RMSE_elimyear_sd,
                      x_label = "grid_size", y_label = "RMSE from eliminating year")
}

```


Map of sampling grid position, sampling grid size

```{r}

chosen_grid_size = 0.75
chosen_lat_offset = 0.25
chosen_lon_offset = 0
chosen_year_oi = 2014

data_subset = cell_sf_tidy[(cell_sf_tidy$intensity == "DC") & (cell_sf_tidy$plant_stat_type == "median"), ]

model_output <- run_OLS(full_data = data_subset, predictors = c("onset", "lat", "year_index", "region"), 
                          y.var = "plant", plant_stat = "median", grid_size = chosen_grid_size, 
                          lat_offset = chosen_lat_offset, lon_offset = chosen_lon_offset, agg_scheme = FALSE, 
                          plot_samples = TRUE, plot_model_evals = FALSE, 
                          year_oi = chosen_year_oi,
                          do_elim_year = FALSE,
                          chosen_intensity = "DC")

```

Model type

```{r}
test_elimyear <- function(elim_year, cell_df, cell_sf, f, model_type, grid_size, plant_stat) {
  
  # eliminate a year and test it; model_type is ols or rf or fe
  train_elimyear <- cell_df[cell_df$year != elim_year,]
  valid_elimyear <- cell_df[cell_df$year == elim_year,]
  train_sf_elimyear <- cell_sf[cell_sf$year != elim_year,] # for ols sampling
  
  # for mapping
  cell_sf_elimyear <- cell_sf[cell_sf$year == elim_year,]
  
  # fit model
  if (model_type == "rf") {
    model <- randomForest(f, data = train_elimyear, importance = TRUE, ntree = 500, mtry = 2)
    
    rmses_elimyear <- rmse(valid_elimyear$plant, predict(model, valid_elimyear))
    errors_elimyear <- mean(predict(model, valid_elimyear) - valid_elimyear$plant)
    lat_offsets <- 0 # placeholder
    lon_offsets <- 0 # placeholder
  }
  
  if (model_type == "ols") {
    
    lat_offsets <- numeric(0)
    lon_offsets <- numeric(0)
    rmses_elimyear <- numeric(0)
    errors_elimyear <- numeric(0)
    
    for (lat_offset in seq(0, 0.75, by = 0.25)) { # 0 to 1.5
      for (lon_offset in seq(0, 0.75, by = 0.25)) { # 0 to 1.5
        train_sampled <- get_sampled_data(full_data = train_sf_elimyear, plant_stat = plant_stat, grid_size = grid_size, 
                                   lat_offset = lat_offset, lon_offset = lon_offset, agg_scheme = FALSE, 
                                   plot_samples = FALSE, year_oi = 2012) # year_oi is for plotting only
        model <- lm(f, data = train_sampled)
        
        rmse_elimyear <- rmse(valid_elimyear$plant, predict(model, valid_elimyear))
        error_elimyear <- mean(predict(model, valid_elimyear) - valid_elimyear$plant)
        
        lat_offsets <- c(lat_offsets, lat_offset)
        lon_offsets <- c(lon_offsets, lon_offset)
        rmses_elimyear <- c(rmses_elimyear, rmse_elimyear)
        errors_elimyear <- c(errors_elimyear, error_elimyear)
      }
    }
    
  }
  
  if (model_type == "fe") {
    train_elimyear_panel <- pdata.frame(train_elimyear,index = c("cell_ID"))
    valid_elimyear_panel <- pdata.frame(valid_elimyear, index = c("cell_ID"))
    
    fe_model <- plm(f_fe, data =  train_elimyear_panel, model = "within")
    fixed_effects <- fixef(fe_model)
    fe_df <- data.frame(fixef_cell_ID = names(fixed_effects), 
                        fixef = as.numeric(fixed_effects))
    
    # ensures that rows are in same order as valid
    temp <- dplyr::left_join(valid_elimyear, fe_df, by = c("cell_ID" = "fixef_cell_ID"))
    
    # get only the cells that have fixed effects predicted
    has_fe_indeces <- which(!is.na(temp$fixef))
    prediction <- temp$fixef + fe_model$coefficients['year'] * valid_elimyear$year +  fe_model$coefficients['onset'] * valid_elimyear$onset
    
    # get only the cells that have fixed effects predicted
    rmses_elimyear <- rmse(valid_elimyear$plant[has_fe_indeces], prediction[has_fe_indeces])
    errors_elimyear <- mean(prediction[has_fe_indeces] - valid_elimyear$plant[has_fe_indeces])
    lat_offsets <- 0 # placeholder
    lon_offsets <- 0 # placeholder
  }

  print('year done')
  
  return(data.frame(elim_year = elim_year,
                    rmse_elimyear = rmses_elimyear,
                    error_elimyear = errors_elimyear,
                    lat_offset = lat_offsets,
                    lon_offset = lon_offsets))

}

test_elimcell <- function(elim_cell, cell_df, cell_sf, model_type, f, grid_size, plant_stat) {
  
  train_elimcell <- cell_df[cell_df$cell_ID != elim_cell,]
  valid_elimcell <- cell_df[cell_df$cell_ID == elim_cell,]
  train_sf_elimcell <- cell_sf[cell_sf$cell_ID != elim_cell,]
  
  # for mapping
  cell_sf_elimcell <- cell_sf[cell_sf$cell_ID == elim_cell, ]
  
  # fit model
  if (model_type == "rf") {
    model <- randomForest(f, data = train_elimcell, importance = TRUE, ntree = 600, mtry = 2)
    
    rmses_elimcell <- rmse(valid_elimcell$plant, predict(model, valid_elimcell))
    errors_elimcell <- mean(predict(model, valid_elimcell) - valid_elimcell$plant)
    lat_offsets <- 0 # placeholder
    lon_offsets <- 0 # placeholder
  }
  
  if (model_type == "ols") {
    lat_offsets <- numeric(0)
    lon_offsets <- numeric(0)
    rmses_elimcell <- numeric(0)
    errors_elimcell <- numeric(0)
    
    for (lat_offset in seq(0, 0.75, by = 0.25)) { # 0 to 1.5
      for (lon_offset in seq(0, 0.75, by = 0.25)) { # 0 to 1.5
        train_sampled <- get_sampled_data(full_data = train_sf_elimcell, plant_stat = plant_stat, grid_size = grid_size, 
                                   lat_offset = lat_offset, lon_offset = lon_offset, agg_scheme = FALSE, 
                                   plot_samples = FALSE, year_oi = 2012) # year_oi is for plotting only
        model <- lm(f, data = train_sampled)
        
        rmse_elimcell <- rmse(valid_elimcell$plant, predict(model, valid_elimcell))
        error_elimcell <- mean(predict(model, valid_elimcell) - valid_elimcell$plant)
        
        lat_offsets <- c(lat_offsets, lat_offset)
        lon_offsets <- c(lon_offsets, lon_offset)
        rmses_elimcell <- c(rmses_elimcell, rmse_elimcell)
        errors_elimcell <- c(errors_elimcell, error_elimcell)
      }
    }
    
  }

  if (model_type == "fe") {
    train_panel <- pdata.frame(train_elimcell,index = c("cell_ID"))
    model <- plm(f, data =  train_panel, model = "within")
    
    rmses_elimcell <- rmse(valid_elimcell$plant, predict(model, valid_elimcell))
    errors_elimcell <- mean(predict(model, valid_elimcell) - valid_elimcell$plant)
  }
  
  print('cell done')
  
  return(data.frame(elim_cell = elim_cell,
                    rmse_elimcell = rmses_elimcell,
                    error_elimcell = errors_elimcell,
                    lat_offset = lat_offsets,
                    lon_offset = lon_offsets))
}


```

```{r}

# for each OLS, FE and RF; and also for each intensity x percentile; and for each grid sampling offset, train and get prediction accuracy for (1) 70%/30% train-test split; (2) prediction at new years; (3) prediction at new locations

do_elimcell <- TRUE
do_elimyear <- TRUE
cell_reducer <- 10 # the number of cells by this when NOT sampling data and doing elimcell

set.seed(100)
wanted_variables <- c('year', 'lat', 'lon', 'onset', 'region', 'intensity', 'plant', 'cell_ID')
wanted_predictors <- c('year', 'lat', 'onset', 'region') # for ols and rf
wanted_predictors_fe <- c('year', 'onset') # for fe
f <- as.formula(paste('plant ~', paste(wanted_predictors, collapse = " + ")))
f_fe <- as.formula(paste('plant ~', paste(wanted_predictors_fe, collapse = " + ") ))

# NOTE, SEPARATE MODELS FOR EACH PERCENTILE AND INTENSITY
chosen_percentile <- "percentile5"
chosen_grid_size <- 0.75
chosen_intensity <- "DC"

cell_sf <- cell_sf_tidy
cell_df <- cell_sf_tidy
st_geometry(cell_df) <- NULL

cell_df <- cell_df[(cell_df$plant_stat_type == chosen_percentile) & (cell_df$intensity == chosen_intensity), ]
cell_df$region <- as.factor(cell_df$region)
cell_sf <- cell_sf[(cell_sf$plant_stat_type == chosen_percentile)  & (cell_df$intensity == chosen_intensity), ]

# to store prediction results
prediction_error_results <- data.frame(model_type = character(0),
                                      elim_type = character(0),
                                      elim_year = numeric(0),
                                      elim_cell = character(0),
                                      rmse = numeric(0),
                                      error = numeric(0),
                                      lat_offset = numeric(0),
                                      lon_offset = numeric(0))

# random forest: regular validaton, 30% randomly eliminated  ----------------------------------------------
train_indeces <- sample(nrow(cell_df), 0.7*nrow(cell_df), replace = FALSE)
train <- cell_df[train_indeces,]
train_sf <- cell_sf[train_indeces,]
valid <- cell_df[-train_indeces,]
  
rf_model1 <- randomForest(f, data = train, importance = TRUE, ntree = 600, mtry = 2)
valid$rf_predicted_plant <- predict(rf_model1, valid)

rf_rmse <- rmse(valid$plant, valid$rf_predicted_plant)
rf_error <- mean(valid$rf_predicted_plant - valid$plant)
  
# save
prediction_error_results <- rbind(prediction_error_results,
                                data.frame(model_type = c('rf'),
                                           elim_type = c('30%'),
                                           elim_year = c(NA),
                                           elim_cell = c(NA),
                                           rmse = c(rf_rmse),
                                           error = c(rf_error),
                                           lat_offset = 0,
                                           lon_offset = 0))
  
# random forest: eliminate year ---------------------------------------------------------------------------
  if (do_elimyear) {
    for (elim_year in 2004:2014) {
      result <- test_elimyear(elim_year, cell_df, cell_sf, f, "rf", grid_size = chosen_grid_size, plant_stat = chosen_percentile)
  
      prediction_error_results <- rbind(prediction_error_results,
  
                                  data.frame(model_type = c('rf'),
                                             elim_type = c('year'),
                                             elim_year = c(elim_year),
                                             elim_cell = c(NA),
                                             rmse = c(result$rmse_elimyear),
                                             error = c(result$error_elimyear),
                                            lat_offset = 0,
                                            lon_offset = 0))
      #print(result)
    }
  }

  # random forest: eliminate cell -------------------------------------------------------------------------
  unique_cells <- unique(cell_df[cell_df$year == 2004, "cell_ID"])

  chosen_cells <- sample(unique_cells, length(unique_cells)/cell_reducer, replace = FALSE) # randomly select some unique cells, otherwise too much data

  cell_sf_elimcell <- cell_sf[cell_sf$year == 2004 & cell_sf$intensity == "DC" &
                                cell_sf$cell_ID %in% chosen_cells, ] # to store error values
  cell_sf_elimcell$error <- rep(NA, nrow(cell_sf_elimcell))  # to store errors

  if (do_elimcell) {
    for (elim_cell in chosen_cells) {
      result <- test_elimcell(elim_cell, cell_df, cell_sf, model_type = "rf", f, grid_size = chosen_grid_size)
  
      # save
      prediction_error_results <- rbind(prediction_error_results,
  
                                  data.frame(model_type = c('rf'),
                                             elim_type = c('cell'),
                                             elim_year = c(NA),
                                             elim_cell = c(elim_cell),
                                             rmse = c(result$rmse_elimcell),
                                             error = c(result$error_elimcell),
                                            lat_offset = 0,
                                            lon_offset = 0))
  
      # for mapping
      cell_sf_elimcell[cell_sf_elimcell$cell_ID == elim_cell, "error"] <- result$error_elimcell[1]
      }
  
    elimcell_error_map <- ggplot(cell_sf_elimcell) +
      geom_sf(aes(fill = error)) +
      scale_fill_viridis() +
      ggtitle(paste("Random forest prediction error, eliminated individual cell")) +
      geom_polygon(data = MT_outline, aes(x = long, y = lat), color = "black", alpha = 0, linetype = 1) +
      theme_bw()
  
    print(elimcell_error_map)
  }
  
  # ols: regular validaton, 30% randomly eliminated  ----------------------------------------------
  
  lat_offsets <- numeric(0)
  lon_offsets <- numeric(0)
  rmses <- numeric(0)
  errors <- numeric(0)
    
  for (lat_offset in seq(0, 0.75, by = 0.25)) { # 0 to 1.5
    for (lon_offset in seq(0, 0.75, by = 0.25)) { # 0 to 1.5
        train_sampled <- get_sampled_data(full_data = train_sf, plant_stat = chosen_percentile, grid_size = chosen_grid_size, 
                                   lat_offset = lat_offset, lon_offset = lon_offset, agg_scheme = FALSE, 
                                   plot_samples = FALSE, year_oi = 2012) # year_oi is for plotting only
        ols_model <- lm(f, data = train_sampled)
        
        rmse <- rmse(valid$plant, predict(ols_model, valid))
        error <- mean(predict(ols_model, valid) - valid$plant)
        
        lat_offsets <- c(lat_offsets, lat_offset)
        lon_offsets <- c(lon_offsets, lon_offset)
        rmses <- c(rmses, rmse)
        errors <- c(errors, error)
      }
  }
  
  # save
  prediction_error_results <- rbind(prediction_error_results,
                                data.frame(model_type = c('ols'),
                                           elim_type = c('30%'),
                                           elim_year = c(NA),
                                           elim_cell = c(NA),
                                           rmse = rmses,
                                           error = errors,
                                           lat_offset = lat_offsets,
                                           lon_offset = lon_offsets))
  
  # ols: eliminate year -----------------------------------------------------------------------------
  
  if (do_elimyear) {
    for (elim_year in 2004:2014) {
      result <- test_elimyear(elim_year, cell_df, cell_sf, f, "ols", grid_size = chosen_grid_size, plant_stat = chosen_percentile)
      prediction_error_results <- rbind(prediction_error_results,
                                      
                                  data.frame(model_type = c('ols'),
                                             elim_type = c('year'),
                                             elim_year = c(elim_year),
                                             elim_cell = c(NA),
                                             rmse = c(result$rmse_elimyear),
                                             error = c(result$error_elimyear),
                                            lat_offset = result$lat_offset,
                                            lon_offset = result$lon_offset))
      # print(result)
    }
  }

  
  # ols: eliminate cell -------------------------------------------------------------------------
  # chosen_cells was defined at rf's eliminate cell section
  cell_sf_elimcell <- cell_sf[cell_sf$year == 2004 & cell_sf$intensity == "DC" &
                                cell_sf$cell_ID %in% chosen_cells, ] # to store error values
  cell_sf_elimcell$error <- rep(NA, nrow(cell_sf_elimcell))  # to store errors

  if (do_elimcell) {
    for (elim_cell in chosen_cells) {
      result <- test_elimcell(elim_cell, cell_df, cell_sf, model_type = "ols", f, grid_size = chosen_grid_size, plant_stat = chosen_percentile)
  
      # save
      prediction_error_results <- rbind(prediction_error_results,
  
                                  data.frame(model_type = c('ols'),
                                             elim_type = c('cell'),
                                             elim_year = c(NA),
                                             elim_cell = c(elim_cell),
                                             rmse = c(result$rmse_elimcell),
                                             error = c(result$error_elimcell),
                                            lat_offset = result$lat_offset,
                                            lon_offset = result$lon_offset))
  
      # print(result)
  
      # for mapping
      cell_sf_elimcell[cell_sf_elimcell$cell_ID == elim_cell, "error"] <- result$error_elimcell[1]
    }
  
    elimcell_error_map <- ggplot(cell_sf_elimcell) +
      geom_sf(aes(fill = error)) +
      scale_fill_viridis() +
      ggtitle(paste("OLS prediction error, eliminated individual cell")) +
      geom_polygon(data = MT_outline, aes(x = long, y = lat), color = "black", alpha = 0, linetype = 1) +
      theme_bw()
  
    print(elimcell_error_map)
  }

# fe validation, elimiante random 30% -----------------------------------------------------------------
train_panel <- pdata.frame(train, index = c("cell_ID"))

fe_model <- plm(f_fe, data =  train_panel, model = "within")
fixed_effects <- fixef(fe_model)
fe_df <- data.frame(fixef_cell_ID = names(fixed_effects), fixef = as.numeric(fixed_effects))

temp <- dplyr::left_join(valid, fe_df, by = c("cell_ID" = "fixef_cell_ID")) # ensures that rows are in same order as valid

# get only the cells that have fixed effects predicted
has_fe_indeces <- which(!is.na(temp$fixef))

prediction <- temp$fixef + fe_model$coefficients['year'] * valid$year +  
              fe_model$coefficients['onset'] * valid$onset

# get only the cells that have fixed effects predicted

fe_rmse <- rmse(valid$plant[has_fe_indeces], prediction[has_fe_indeces])
fe_error <- mean(prediction[has_fe_indeces] - valid$plant[has_fe_indeces])

# save
prediction_error_results <- rbind(prediction_error_results,
                                data.frame(model_type = c('fe'),
                                           elim_type = c('30%'),
                                           elim_year = c(NA),
                                           elim_cell = c(NA),
                                           rmse = c(fe_rmse),
                                           error = c(fe_error),
                                           lat_offset = 0,
                                           lon_offset = 0))
    
# fe: elimyear validaton ----------------------------------------------

if (do_elimyear) {
  for (elim_year in 2004:2014) {
      
    result <- test_elimyear(elim_year, cell_df, cell_sf, f_fe, "fe", grid_size = chosen_grid_size, plant_stat = chosen_percentile)
    prediction_error_results <- rbind(prediction_error_results,
                                      
                                  data.frame(model_type = c('fe'),
                                             elim_type = c('year'),
                                             elim_year = c(elim_year),
                                             elim_cell = c(NA),
                                             rmse = c(result$rmse_elimyear),
                                             error = c(result$error_elimyear),
                                            lat_offset = result$lat_offset,
                                            lon_offset = result$lon_offset))
      # print(result)
  }
}

prediction_error_results$intensity <- chosen_intensity
prediction_error_results$percentile <- chosen_percentile

saveRDS(prediction_error_results, paste0("prediction_error_results_", onset_type, '_', chosen_intensity, '_', chosen_percentile, ".rds"))

```

Plot of prediction accuracy metrics for each model type

```{r}

# REPEAT FOR EACH INTENSITY AND PERCENTILE

imported_prediction_error <- readRDS('prediction_error_results_freq_10_persiann_DC_percentile5.rds')

# rmse vs eliminated year, summarized for different intensities and percentiles and sampling grid locations

elimyear_results <- imported_prediction_error[imported_prediction_error$elim_type == "year",]
elimyear_rf_fe <- elimyear_results[elimyear_results$model_type == 'rf' | elimyear_results$model_type == 'fe',]
summarized_elimyear_rf_fe <- elimyear_rf_fe[, c('elim_year', 'rmse', 'error')]
summarized_elimyear_rf_fe$rmse_sd <- 0
summarized_elimyear_rf_fe$error_sd <- 0
summarized_elimyear_rf_fe$model_type <- elimyear_rf_fe$model_type
colnames(summarized_elimyear_rf_fe) <- c('elim_year', 'rmse_mean', 'error_mean', 'rmse_sd', 'error_sd', 'model_type')

elimyear_ols <- elimyear_results[elimyear_results$model_type == 'ols',]

summarized_elimyear_ols <- elimyear_ols %>% 
  group_by(elim_year) %>% 
  dplyr::summarize(rmse_mean = mean(rmse),
                   error_mean = mean(error),
                   rmse_sd = sd(rmse),
                   error_sd = sd(error)) %>%
  mutate(model_type = 'ols')

summarized_elimyear <- rbind(summarized_elimyear_rf_fe, summarized_elimyear_ols)

ggplot(summarized_elimyear, aes(x = elim_year, y = rmse_mean, col = model_type)) +
  geom_line() +
  geom_errorbar(aes(ymin=rmse_mean - rmse_sd, ymax=rmse_mean + rmse_sd), width=0) +
  ggtitle('eliminated year, rmse') + 
  theme_bw()

ggplot(summarized_elimyear, aes(x = elim_year, y = error_mean, col = model_type)) +
  geom_line() +
  geom_errorbar(aes(ymin=error_mean - error_sd, ymax=error_mean + error_sd), width=0) +
  ggtitle('eliminated year, error') + 
  theme_bw()

# rmse vs eliminated cell

elimcell_results <- imported_prediction_error[imported_prediction_error$elim_type == "cell",]
elimcell_rf <- elimcell_results[elimcell_results$model_type == 'rf',]
elimcell_ols <- elimcell_results[elimcell_results$model_type == 'ols',]

print('_____')
print(paste('elim cell, rmse, rf: ', mean(elimcell_rf$rmse), '+/-', sd(elimcell_rf$rmse)))
print(paste('elim cell, rmse, ols: ', mean(elimcell_ols$rmse), '+/-', sd(elimcell_ols$rmse)))
print('_____')
print(paste('elim cell, error, rf: ', mean(elimcell_rf$error), '+/-', sd(elimcell_rf$error)))
print(paste('elim cell, error, ols: ', mean(elimcell_ols$error), '+/-', sd(elimcell_ols$error)))

# calculate mean rmse for all prediction accuracies

elimrandom_results <- imported_prediction_error[imported_prediction_error$elim_type == "30%",]
elimrandom_rf <- elimrandom_results[elimrandom_results$model_type == 'rf',]
elimrandom_ols <- elimrandom_results[elimrandom_results$model_type == 'ols',]
elimrandom_fe <- elimrandom_results[elimrandom_results$model_type == 'fe',]

print('_____')
print(paste('elim 30%, rmse, rf: ', mean(elimrandom_rf$rmse), '+/-', sd(elimrandom_rf$rmse)))
print(paste('elim 30%, rmse, ols: ', mean(elimrandom_ols$rmse), '+/-', sd(elimrandom_ols$rmse)))
print(paste('elim 30%, rmse, fe: ', mean(elimrandom_fe$rmse), '+/-', sd(elimrandom_fe$rmse)))
print('_____')
print(paste('elim 30%, error, rf: ', mean(elimrandom_rf$error), '+/-', sd(elimrandom_rf$error)))
print(paste('elim 30%, error, ols: ', mean(elimrandom_ols$error), '+/-', sd(elimrandom_ols$error)))
print(paste('elim 30%, error, fe: ', mean(elimrandom_fe$error), '+/-', sd(elimrandom_fe$error)))
```

```{r}

# F test for FE vs pooled OLS

# set up models with all data
chosen_percentile <- "percentile5"
chosen_intensity <- "DC"

cell_df <- cell_sf_tidy[(cell_sf_tidy$plant_stat_type == chosen_percentile) & (cell_sf_tidy$intensity == chosen_intensity), ]
cell_df_panel <- pdata.frame(cell_df, index = c("cell_ID"))

ols_model <- lm(plant ~ onset + year + lat + region, data = cell_df)

fe_model <- plm(plant ~ onset + year, data =  cell_df_panel, model = "within")

print(plm::pFtest(fe_model, ols_model))

```

Model evaluation

```{r}

# for the chosen FE specification, but for each intensity x percentile , do model evaluation:

model_evaluation <- data.frame(intensity = character(0), percentile = character(0),
                               onset_coef = numeric(0), onset_coef_stderr = numeric(0),
                               year_coef = numeric(0), year_coef_stderr = numeric(0),
                               R2 = numeric(0), resid_mean = numeric(0), 
                               percent_tempauto = numeric(0), mean_residual_moran = numeric(0), 
                               mean_residual_moran_pval = numeric(0),
                               highest_predictor_correlation = numeric(0)
                               )

for (intensity in c("DC", "SC")) {
  for (percentile in c("percentile5", "percentile25", "median")) {
    cell_df <- cell_sf_tidy
    st_geometry(cell_df) <- NULL
    
    data_subset_sf <- cell_sf_tidy[(cell_sf_tidy$plant_stat_type == percentile) & (cell_sf_tidy$intensity == intensity), ]
    
    data_subset <- cell_df[(cell_df$plant_stat_type == percentile) & (cell_df$intensity == intensity), ]
    data_subset_panel <- pdata.frame(data_subset, index = c("cell_ID"))

    fe_model <- plm(plant ~ onset + year, data = data_subset_panel, model = "within")
    
    data_subset_sf$residuals <- residuals(fe_model)
    data_subset_sf$fitted.values <- fitted.values(fe_model)
    data_subset$residuals <- residuals(fe_model)
    data_subset$fitted.values <- fitted.values(fe_model)
    
    # get coefficients
    coefficients <- fe_model$coefficients
    onset_coef <- coefficients["onset"]
    year_coef <- coefficients["year"]
    
    # standard errors
    std_errors <- summary(fe_model)$coefficients[,2]
    onset_std_error <- std_errors["onset"]
    year_std_error <- std_errors["year"]
  
    # R2
    SST <- sum((data_subset$plant - mean(data_subset$plant))^2)
    SSE <- sum((data_subset$residuals - mean(data_subset$residuals))^2)
    R2 <- 1 - SSE/SST
    
    # residuals of zero mean --------------------------------------------------------------------
    resid_mean = mean(data_subset$residuals)

    # QQ plot for residuals ---------------------------------------------------------------------
    qqnorm(data_subset$residuals, pch = 1, frame = FALSE, main = paste('QQ plot for residual', intensity, percentile))
    qqline(data_subset$residuals, col = "steelblue", lwd = 2)
    
    # correlation between residual and each of the predictors (exogeneity) ----------------------
    plot(data_subset$onset, data_subset$residuals, main = paste("onset vs residual (exogeneity)", intensity, percentile))
    abline(h = 0)
    
    plot(data_subset$year, data_subset$residuals, main = paste("year vs residual (exogeneity)", intensity, percentile)) 
    abline(h = 0)
    
    # temporal autocorrelation of residual -----------------------------------------------------
    data_subset_2 <- data_subset[complete.cases(data_subset),]
    cells_list <- list()
    i <- 1
    for (year in 2004:2014) {
       cells_in_year <- data_subset_2[data_subset_2$year == year,]
       cells_list[[i]] <- cells_in_year$label
       i <- i + 1
    }
    full_data_cells <- Reduce(intersect, cells_list)
    data_subset_2 <- data_subset_2[data_subset_2$label %in% full_data_cells, ]
     
    data_subset_2_nested <- group_by(data.frame(data_subset_2), label) %>% nest()
     
    dwtest_one_cell <- function(data) {
      dwtest(residuals ~ 1, data = data)
     }
     
    data_subset_2 <- data_subset_2_nested %>%
       mutate(dwtest = map(data, dwtest_one_cell)) %>%
       mutate(test_df = map(dwtest, generics::tidy)) %>%
       unnest(test_df)
     
    # calculate proportion of p values below 5% significance, with Bonferroni correction
    percent_tempauto <- mean(data_subset_2$p.value < 0.05/nrow(data_subset_2))
    
    # spatial autocorrelation of residual ----------------------------------------------------------
    
    residual_morans <- numeric(0)
    residual_moran_pvals <- numeric(0)
    
    for (year_oi in 2004:2014) {
      # need 'one layer': one year, one intensity, set up weights
      to_autocorrelation <- data_subset_sf[data_subset_sf$year == year_oi, ]
      to_autocorrelation_sp <- as(to_autocorrelation, "Spatial")
      st_geometry(to_autocorrelation) <- NULL # turn to_autocorrelation into data frame
      centroids <- coordinates(to_autocorrelation_sp)
      to_autocorrelation_points <- SpatialPointsDataFrame(coords = centroids, data = to_autocorrelation)
      nb<-knn2nb(knearneigh(to_autocorrelation_points)) 
      lw <- nb2listw(nb, zero.policy = TRUE)
      
      # calculate spatial autocorrelation
      moran_residual <- moran.mc(to_autocorrelation_points$residuals, lw, 500, zero.policy = TRUE)
      
      residual_morans <- c(residual_morans, moran_residual$statistic)
      residual_moran_pvals <- c(residual_moran_pvals, moran_residual$p.value)
    }
    
    mean_residual_moran <- mean(residual_morans)
    mean_residual_moran_pval <- mean(residual_moran_pvals)
    
    # correlation of selected predictors (to each other) -------------------------------------------
    predictors <- data_subset[,c("onset", "year")]
    correlations <- cor(predictors)
    diag(correlations) <- 0 # place 1 with 0 on diagonal
    highest_predictor_correlation <- max(abs(correlations))
    
    # residual vs fitted value plot (look for no pattern and constant variance) --------------------
    plot(x = as.numeric(data_subset$fitted.values), y = as.numeric(data_subset$residuals), 
         main = paste("fitted values vs residuals", intensity, percentile))
    
    # residual vs index (look for no pattern) ------------------------------------------------------
    plot(as.numeric(data_subset$residuals), main = paste("residual vs index", intensity, percentile))
    
    # save results
    model_evaluation <- rbind(model_evaluation, data.frame(intensity = intensity, percentile = percentile,
                               onset_coef = onset_coef, onset_coef_stderr = onset_std_error,
                               year_coef = year_coef, year_coef_stderr = year_std_error,
                               R2 = R2, resid_mean = resid_mean, 
                               percent_tempauto = percent_tempauto, mean_residual_moran = mean_residual_moran, 
                               mean_residual_moran_pval = mean_residual_moran_pval,
                               highest_predictor_correlation = highest_predictor_correlation
                               ))
  }
}

print(model_evaluation)

```


Map of fixed effects that were estimated

```{r}

# try year_index to get 
# use all cells for the FE model; produce maps for SC and DC separately
for (intensity in c("DC", "SC")) {
  for (percentile in c("percentile5", "percentile25", "median")) {
    
    cell_df <- cell_sf_tidy
    st_geometry(cell_df) <- NULL
    
    data_subset_sf <- cell_sf_tidy[(cell_sf_tidy$plant_stat_type == percentile) & (cell_sf_tidy$intensity == intensity), ]
    
    data_subset <- cell_df[(cell_df$plant_stat_type == percentile) & (cell_df$intensity == intensity), ]
    data_subset_panel <- pdata.frame(data_subset, index = c("cell_ID"))

    fe_model <- plm(plant ~ onset + year_index, data = data_subset_panel, model = "within")
    fixed_effects <- fixef(fe_model)
    
    fe_df <- data.frame(cell_ID = names(fixed_effects),
                    fe = fixed_effects)
    # join
    fe_sf <- merge(data_subset_sf, fe_df, by = "cell_ID")

    # plot fe
    year <- 2014 # all years will be the same for fixed effects
    fe_sf_year <- fe_sf[fe_sf$year == year, ]

    fe_map <- ggplot(fe_sf_year) +
      geom_sf(aes(fill = fe), colour = NA) +
      scale_fill_viridis() +
      ggtitle(paste("Fixed effects", intensity, percentile)) +
      geom_polygon(data = MT_outline, aes(x = long, y = lat), color = "black", alpha = 0, linetype = 1) +
      theme_bw()
    
    print(fe_map)
  }
}


```

Map of residual for OLS and FE (can't do predictive error map for FE)

```{r}

# residual map: use all cells for OLS and FE models; produce maps for SC and DC separately ----------------------------

for (intensity in c("SC", "DC")) {
  for (percentile in c('percentile5', 'percentile25', 'median')) {
    cell_df <- cell_sf_tidy
    st_geometry(cell_df) <- NULL
        
    data_subset_sf <- cell_sf_tidy[(cell_sf_tidy$plant_stat_type == percentile) & (cell_sf_tidy$intensity == intensity), ]
        
    data_subset <- cell_df[(cell_df$plant_stat_type == percentile) & (cell_df$intensity == intensity), ]
    data_subset_panel <- pdata.frame(data_subset, index = c("cell_ID"))
    
    fe_model <- plm(plant ~ onset + year, data = data_subset_panel, model = "within")
    ols_model <- lm(plant ~ onset + year + lat + region, data = data_subset)
    
    data_subset_sf$ols_residuals <- ols_model$residuals
    data_subset_sf$fe_residuals <- fe_model$residuals
    
    year <- 2014
    data_subset_sf_year <- data_subset_sf[data_subset_sf$year == year, ]
        
    ols_resid_map <- ggplot(data_subset_sf_year) +
      geom_sf(aes(fill = ols_residuals), color = NA) +
      scale_fill_viridis() +
      ggtitle(paste("OLS residuals", year, intensity, percentile)) +
      geom_polygon(data = MT_outline, aes(x = long, y = lat), color = "black", alpha = 0, linetype = 1) +
      theme_bw()
    
    fe_resid_map <- ggplot(data_subset_sf_year) +
      geom_sf(aes(fill = fe_residuals), color = NA) +
      scale_fill_viridis() +
      ggtitle(paste("FE residuals", year, intensity, percentile)) +
      geom_polygon(data = MT_outline, aes(x = long, y = lat), color = "black", alpha = 0, linetype = 1) +
      theme_bw()
    
    print(ols_resid_map)
    print(fe_resid_map)
  }
}

# predictive error map, eliminate one year, for OLS and FE ----------------------------------------------

elim_year = 2010

for (intensity in c("SC", "DC")) {
  for (percentile in c('percentile5', 'percentile25', 'median')) {
    
    # eliminate a year and test it; model_type is ols or rf or fe
    train_elimyear <- data_subset[data_subset$year != elim_year,]
    valid_elimyear <- data_subset[data_subset$year == elim_year,]
      
    # for mapping
    valid_sf_elimyear <- data_subset_sf[data_subset_sf$year == elim_year,]
    
    # ols model
    ols_model <- lm(plant ~ onset + year + lat + region, data = train_elimyear)
    ols_error_elimyear <- predict(ols_model, valid_elimyear) - valid_elimyear$plant
    valid_sf_elimyear$ols_error <- ols_error_elimyear
    
    # fe model
    train_elimyear_panel <- pdata.frame(train_elimyear,index = c("cell_ID"))

    fe_model <- plm(plant ~ onset + year, data =  train_elimyear_panel, model = "within")
    fixed_effects <- fixef(fe_model)
    fe_df <- data.frame(fixef_cell_ID = names(fixed_effects), 
                        fixef = as.numeric(fixed_effects))
        
    # ensures that rows are in same order as valid
    temp <- dplyr::left_join(valid_elimyear, fe_df, by = c("cell_ID" = "fixef_cell_ID"))
        
    # get only the cells that have fixed effects predicted
    has_fe_indeces <- which(!is.na(temp$fixef))
    prediction <- temp$fixef + fe_model$coefficients['year'] * valid_elimyear$year +  fe_model$coefficients['onset'] * valid_elimyear$onset
        
    # get only the cells that have fixed effects predicted
    fe_error_elimyear <- prediction[has_fe_indeces] - valid_elimyear$plant[has_fe_indeces]
    valid_sf_elimyear_fe <- valid_sf_elimyear[has_fe_indeces,]
    valid_sf_elimyear_fe$fe_error <- fe_error_elimyear
    
    # map
    ols_prediction_map <- ggplot(valid_sf_elimyear) +
      geom_sf(aes(fill = ols_error), color = NA) +
      scale_fill_viridis() +
      ggtitle(paste("OLS prediction error", elim_year, intensity, percentile)) +
      geom_polygon(data = MT_outline, aes(x = long, y = lat), color = "black", alpha = 0, linetype = 1) +
      theme_bw()
    
    fe_prediction_map <- ggplot(valid_sf_elimyear_fe) +
      geom_sf(aes(fill = fe_error), color = NA) +
      scale_fill_viridis() +
      ggtitle(paste("FE prediction error", elim_year, intensity, percentile)) +
      geom_polygon(data = MT_outline, aes(x = long, y = lat), color = "black", alpha = 0, linetype = 1) +
      theme_bw()
    
    print(ols_prediction_map)
    print(fe_prediction_map)
  }
}
```

Model results

```{r}

# onset and year coef for each intensity and percentile, with uncertainty in coef due to planting date estimation error and standard error

plant_sd <- 6.9 # error
n_data_bootstrap <- 1000 # number of bootstrap samples to take of planting date estimates

# save data bootstrap results
data_bootstrap_coefs <- data.frame(intensity = character(0), percentile = character(0),
                                   onset_coef = numeric(0), onset_sderr = numeric(0),
                                   year_coef = numeric(0), year_sderr = numeric(0))

# take one bootstramp sample of planting date, assuming normal distribution
bootstrap_one_date <- function(row, sd) {
  mean_date <- row$plant
  row$bootstrap_plant <- rnorm(1, mean = mean_date, sd = sd)
}

cell_df <- cell_sf_tidy
st_geometry(cell_df) <- NULL

for (n in 1:n_data_bootstrap) {
  print(n)
  # Step 1. bootstrap planting date information
  bs_plant <- apply(cell_sf_tidy,1,bootstrap_one_date,sd = plant_sd)
  bs_cell_df <- cell_df
  bs_cell_df$bs_plant <- bs_plant
  
  # Step 2. split by intensity, percentile
  for (intensity in c("SC", "DC")) {
    for (percentile in c('percentile5', 'percentile25', 'median')) {
      

      bs_data_subset <- bs_cell_df[(bs_cell_df$plant_stat_type == percentile) & (bs_cell_df$intensity == intensity), ]
      bs_data_subset_panel <- pdata.frame(bs_data_subset, index = c("cell_ID"))
          
      # Step 3. run FE model, get coefficients
      fe_model <- plm(plant ~ onset + year, data = bs_data_subset_panel, model = "within")
      
      # get coefficients
      coefficients <- fe_model$coefficients
      onset_coef <- coefficients["onset"]
      year_coef <- coefficients["year"]
          
      # standard errors
      std_errors <- summary(fe_model)$coefficients[,2]
      onset_std_error <- std_errors["onset"]
      year_std_error <- std_errors["year"]
      
      data_bootstrap_coefs <- rbind(data_bootstrap_coefs, data.frame(intensity = intensity, percentile = percentile,
                                   onset_coef = onset_coef, onset_sderr = onset_std_error,
                                   year_coef = year_coef, year_sderr = year_std_error))
    }
  }
}

```

```{r}

final_bootstrap_coefs <- data.frame(intensity = character(0), percentile = character(0),
                                    onset_coef = numeric(0), onset_sderr = numeric(0),
                                    year_coef = numeric(0), year_sderr = numeric(0))

bootstrap_one_coef <- function(row, mean_coef_name, sd_coef_name, n) {
  mean_coef <- as.numeric(row[mean_coef_name])
  sd_coef <- as.numeric(row[sd_coef_name])
  bs_coefs <- rnorm(n = n, mean = mean_coef, sd = sd_coef)
}

onset_coefs_list <- vector("list", 6)
year_coefs_list <- vector("list", 6)

list_index <- 0
for (intensity in c("SC", "DC")) {
  for (percentile in c('percentile5', 'percentile25', 'median')) {
    
    list_index <- list_index + 1

    coefs <- data_bootstrap_coefs[(data_bootstrap_coefs$intensity == intensity) & (data_bootstrap_coefs$percentile == percentile),]
    
    # these are matrices, rows = number of bootstraps from the standard error distribution
    bs_onset_coefs <- apply(coefs,1,bootstrap_one_coef, mean_coef_name = 'onset_coef', sd_coef_name = 'onset_sderr', n = 10)
    bs_year_coefs <- apply(coefs,1,bootstrap_one_coef, mean_coef_name = 'year_coef', sd_coef_name = 'year_sderr', n = 10)
    
    final_onset_coef <- mean(bs_onset_coefs)
    final_onset_sderr <- sd(bs_onset_coefs)
    final_year_coef <- mean(bs_year_coefs)
    final_year_sderr <- sd(bs_year_coefs)
    
    onset_coefs_list[[list_index]] <- as.vector(bs_onset_coefs)
    year_coefs_list[[list_index]] <- as.vector(bs_year_coefs)
    
    final_bootstrap_coefs <- rbind(final_bootstrap_coefs, data.frame(intensity = intensity, percentile = percentile,
                                                                     onset_coef = final_onset_coef, onset_sderr = final_onset_sderr,
                                                                     year_coef = final_year_coef, year_sderr = final_year_sderr))
  }
}

names(onset_coefs_list) <- c('SC_percentile5', 'SC_percentile25', 'SC_median', 'DC_percentile5', 'DC_percentile25', 'DC_median')
names(year_coefs_list) <- c('SC_percentile5', 'SC_percentile25', 'SC_median', 'DC_percentile5', 'DC_percentile25', 'DC_median')

# compare coefs across intensities and groups
diff_intensities_median <- t.test(onset_coefs_list[['SC_median']], onset_coefs_list[['DC_median']])$p.value
diff_intensities_percentile25 <- t.test(onset_coefs_list[['SC_percentile25']], onset_coefs_list[['DC_percentile25']])$p.value
diff_intensities_percentile5 <- t.test(onset_coefs_list[['SC_percentile5']], onset_coefs_list[['DC_percentile5']])$p.value

diff_percentile5_25_SC <- t.test(onset_coefs_list[['SC_percentile5']], onset_coefs_list[['SC_percentile25']])$p.value
diff_percentile5_25_DC <- t.test(onset_coefs_list[['DC_percentile5']], onset_coefs_list[['DC_percentile25']])$p.value

diff_percentile25_50_SC <- t.test(onset_coefs_list[['SC_percentile25']], onset_coefs_list[['SC_median']])$p.value
diff_percentile25_50_DC <- t.test(onset_coefs_list[['DC_percentile25']], onset_coefs_list[['DC_median']])$p.value

print('p values comparing intensities:')
print(paste(diff_intensities_median, diff_intensities_percentile25, diff_intensities_percentile5))

print('p values comparing percentile5 and percentile25:')
print(paste(diff_percentile5_25_SC, diff_percentile5_25_DC))

print('p values comparing percentile25 and median:')
print(paste(diff_percentile25_50_SC, diff_percentile25_50_DC))

print(final_bootstrap_coefs)

```
