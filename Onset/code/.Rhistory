onset_data <- data[,c('onset_median', 'latitude_median', 'longitude_median', 'year_median')]
onset_data$onset_type <- onset_name
names(onset_data) <- c('onset', 'lat', 'lon', 'year', 'onset_type')
all_onset_data <- rbind(all_onset_data, onset_data)
}
read.csv(filename_median)
filename_median
# CSV DATA ------------------------------------------------------------------------------
# median cell for onset of interest
median_cell <- median_cell_raw %>% delete_cols_median_cell() %>%
rename_cols_median_cell()
median_cell$plant_stat_type <- rep('median', nrow(median_cell))
# add onset data from different onset definitions
all_onset_data <- data.frame(onset = numeric(0),
lat = numeric(0),
lon = numeric(0),
year = numeric(0),
onset_type = character(0))
for (onset_name in all_onset_types) {
if (onset_name != 'Gabriel_onset') {
filename_median <- paste0(first_folder,'/R-code-large-files/data_onset_', onset_name,
'/median_onset_cell_', onset_name, '.csv')
data <- read.csv(filename_median)
}
if (onset_name == 'Gabriel_onset') {
data <- read.csv(first_folder,'/R-code2/Modeling/data/median_onset_cell_v2.csv')
}
onset_data <- data[,c('onset_median', 'latitude_median', 'longitude_median', 'year_median')]
onset_data$onset_type <- onset_name
names(onset_data) <- c('onset', 'lat', 'lon', 'year', 'onset_type')
all_onset_data <- rbind(all_onset_data, onset_data)
}
read.csv(first_folder,'/R-code2/Modeling/data/median_onset_cell_v2.csv')
filename_median
library(ggplot2)
library(tidyverse)
library(broom)
library(dplyr)
library(rgdal)
library(rgeos)
library(raster)
library(sf)
library(sp)
library(tmap)
library(viridis)
library(spdep)
library(Metrics) # for rmse
library(leaflet)
os_system <- 'windows_laptop' # mac for laptop or windows for desktop
if (os_system == 'windows') {first_folder <- 'E:'}
if (os_system == 'mac') {first_folder <- '~/Documents'}
if (os_system == 'windows_laptop') {first_folder <- 'D:'}
source(paste0(first_folder,'/R-code2/Modeling/code/FCN_clean_csvs.R'))
source(paste0(first_folder,'/R-code2/Modeling/code/FCN_plotting.R'))
source(paste0(first_folder,'/R-code2/Modeling/code/FCN_sample_data.R'))
onset_type <- 'thres_10_persiann' #'AA_25_chirps_ATchirps5km' # 'Gabriel_onset'
if (onset_type != 'Gabriel_onset') {
filename_median <- paste0(first_folder,'/R-code-large-files/data_onset_', onset_type, '/median_onset_cell_', onset_type, '.csv')
filename_shp<- paste0(first_folder,'/R-code-large-files/data_onset_', onset_type, '/shp')
layername_shp <- paste0('median_onset_cell_SHP_', onset_type)
median_cell_raw <- read.csv(filename_median)
cell_sf <- st_read(dsn = filename_shp, layer = layername_shp)
}
if (onset_type == 'Gabriel_onset') {
median_cell_raw <- read.csv(paste0(first_folder,'/R-code2/Modeling/data/median_onset_cell_v2.csv'))
cell_sf <- st_read(dsn = paste0(first_folder,'/R-code2/Modeling/data/shp/median_onset_cell'), layer = 'median_onset_cell_SHP')
}
MT_outline <- readOGR(dsn = paste0(first_folder,'/R-code2/Modeling/data/shp/MatoGrossoOutline'), layer = 'MatoGrossoOutline')
crs(MT_outline) <- CRS("+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0")
grid_1deg <- readOGR(dsn = paste0(first_folder,'/R-code2/Modeling/data/shp/grid_1deg'), layer = 'grid_1deg')
munis <- readOGR(dsn = paste0(first_folder,'/R-code2/Modeling/data/shp/munis'), layer = 'munis_SHP')
crs(munis) <- CRS("+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0")
# read in all the onsets. only the csv, not shp
all_onset_types <- c('Gabriel_onset', 'AA_2_persiann', 'AA_3_persiann',
'AA_25_persiann', 'AA_25_chirps_ATgabriel25km', 'AA_25_persiann_ATgabriel25km',
'freq_5_persiann', 'freq_8_persiann', 'freq_10_persiann', 'freq_12_persiann',
'monsoon_30_persiann', 'monsoon_40_persiann',
'pentad_8_persiann', 'pentad_10_persiann', 'pentad_15_persiann',
'rangethres_10_15_15_30_persiann', 'rangethres_10_15_15_40_persiann',
'rangethres_10_15_20_20_persiann', 'rangethres_10_15_20_30_persiann',
'rangethres_10_15_30_40_persiann', 'thres_10_persiann')
# didn't do:  'AA_25_chirps_ATchirps5km'
# CSV DATA ------------------------------------------------------------------------------
# median cell for onset of interest
median_cell <- median_cell_raw %>% delete_cols_median_cell() %>%
rename_cols_median_cell()
median_cell$plant_stat_type <- rep('median', nrow(median_cell))
# add onset data from different onset definitions
all_onset_data <- data.frame(onset = numeric(0),
lat = numeric(0),
lon = numeric(0),
year = numeric(0),
onset_type = character(0))
for (onset_name in all_onset_types) {
if (onset_name != 'Gabriel_onset') {
filename_median <- paste0(first_folder,'/R-code-large-files/data_onset_', onset_name,
'/median_onset_cell_', onset_name, '.csv')
data <- read.csv(filename_median)
}
if (onset_name == 'Gabriel_onset') {
data <- read.csv(first_folder,'/R-code2/Modeling/data/median_onset_cell_v2.csv')
}
onset_data <- data[,c('onset_median', 'latitude_median', 'longitude_median', 'year_median')]
onset_data$onset_type <- onset_name
names(onset_data) <- c('onset', 'lat', 'lon', 'year', 'onset_type')
all_onset_data <- rbind(all_onset_data, onset_data)
}
setwd("D:/R-code2/Onset/code")
filename_median <- paste0(first_folder,'/R-code-large-files/data_onset_', onset_name,
'/median_onset_cell_', onset_name, '.csv')
data <- read.csv(filename_median)
first_folder
filename_median
onset_name
# CSV DATA ------------------------------------------------------------------------------
# median cell for onset of interest
median_cell <- median_cell_raw %>% delete_cols_median_cell() %>%
rename_cols_median_cell()
median_cell$plant_stat_type <- rep('median', nrow(median_cell))
# add onset data from different onset definitions
all_onset_data <- data.frame(onset = numeric(0),
lat = numeric(0),
lon = numeric(0),
year = numeric(0),
onset_type = character(0))
for (onset_name in all_onset_types) {
print('onset_name')
print(onset_name)
if (onset_name != 'Gabriel_onset') {
filename_median <- paste0(first_folder,'/R-code-large-files/data_onset_', onset_name,
'/median_onset_cell_', onset_name, '.csv')
data <- read.csv(filename_median)
}
if (onset_name == 'Gabriel_onset') {
data <- read.csv(first_folder,'/R-code2/Modeling/data/median_onset_cell_v2.csv')
}
onset_data <- data[,c('onset_median', 'latitude_median', 'longitude_median', 'year_median')]
onset_data$onset_type <- onset_name
names(onset_data) <- c('onset', 'lat', 'lon', 'year', 'onset_type')
all_onset_data <- rbind(all_onset_data, onset_data)
}
# CSV DATA ------------------------------------------------------------------------------
# median cell for onset of interest
median_cell <- median_cell_raw %>% delete_cols_median_cell() %>%
rename_cols_median_cell()
median_cell$plant_stat_type <- rep('median', nrow(median_cell))
# add onset data from different onset definitions
all_onset_data <- data.frame(onset = numeric(0),
lat = numeric(0),
lon = numeric(0),
year = numeric(0),
onset_type = character(0))
for (onset_name in all_onset_types) {
print('onset_name')
print(onset_name)
if (onset_name != 'Gabriel_onset') {
filename_median <- paste0(first_folder,'/R-code-large-files/data_onset_', onset_name,
'/median_onset_cell_', onset_name, '.csv')
data <- read.csv(filename_median)
}
if (onset_name == 'Gabriel_onset') {
data <- read.csv(paste0(first_folder,'/R-code2/Modeling/data/median_onset_cell_v2.csv'))
}
onset_data <- data[,c('onset_median', 'latitude_median', 'longitude_median', 'year_median')]
onset_data$onset_type <- onset_name
names(onset_data) <- c('onset', 'lat', 'lon', 'year', 'onset_type')
all_onset_data <- rbind(all_onset_data, onset_data)
}
all_onset_data$year_factor <- as.factor(all_onset_data$year)
# SF DATA -------------------------------------------------------------------------------
# get cell_ID column for median
cell_sf$cell_ID <- median_cell$cell_ID
cell_sf$cell_ID <- sapply(as.character(cell_sf$cell_ID), clean_cell_ID)
# join median, percentile data to cell_sf
# cell_sf has median information, but copy it and put in percentile info for DC and SC plant
cell_sf$plant_stat_type <- rep("median", nrow(cell_sf))
cell_sf_tidy <- cell_sf %>% #tidy_by_intensity_plant("SC_plant", "DC_plant") %>%
#tidy_by_intensity_delay("SC_delay", "DC_delay") %>%
dplyr::select(-c(SC_harvest, DC_harvest, SC_plant, DC_plant)) #%>%
#categorize_regions_cell_sf_tidy() # categorize cells into four regions
cell_sf_tidy$year_index <- cell_sf_tidy$year - 2003
cell_sf_tidy$year_factor <- as.factor(cell_sf_tidy$year)
cell_sf_tidy <- cell_sf_tidy %>%  drop_na
# calculate quantiles of onset
onset_oi_data <- cell_sf_tidy %>%
mutate(onset_quantile_allyears = ntile(onset, 10))
# boxplot
ggplot(onset_oi_data) +
geom_boxplot(aes(x=year_factor, y=onset)) +
ggtitle(onset_type) +
ylim(0, 130) +
theme_bw()
# map which cells belong to which quantile
quantile_maps <- ggplot(onset_oi_data) +
geom_sf(aes(fill = onset_quantile_allyears), color = NA) +
#scale_fill_viridis() +
scale_fill_gradient(low="black", high="red") +
#scale_fill_brewer(palette="YlOrRd") +
ggtitle(paste("Onset quantile for all years, pooled,", onset_type)) +
geom_polygon(data = MT_outline, aes(x = long, y = lat), color = "black",
alpha = 0, linetype = 1) +
facet_wrap(~year_factor) +
theme_bw()
print(quantile_maps)
onset_oi_data <- onset_oi_data %>% group_by(year_factor) %>%
mutate(onset_quantile_eachyear = ntile(onset, 10))
quantile_maps <- ggplot(onset_oi_data) +
geom_sf(aes(fill = onset_quantile_eachyear), color = NA) +
#scale_fill_viridis() +
scale_fill_gradient(low="black", high="blue") +
#scale_fill_brewer(palette="YlOrRd") +
ggtitle(paste("Onset quantile for individual years,", onset_type)) +
geom_polygon(data = MT_outline, aes(x = long, y = lat), color = "black",
alpha = 0, linetype = 1) +
facet_wrap(~year_factor) +
theme_bw()
print(quantile_maps)
onset_oi_data <- onset_oi_data %>% ungroup()
# calculate time-averaged quantiles
# get cells from 2004, filter out all other cells from the other years
cellID_2004 <- onset_oi_data[onset_oi_data$year == 2004, "cell_ID"]
st_geometry(cellID_2004) <- NULL
onset_oi_data_subset <- onset_oi_data[onset_oi_data$cell_ID %in% cellID_2004$cell_ID,]
# group by cell_ID and calculate mean onset
onset_oi_summarized <- onset_oi_data_subset %>% group_by(cell_ID) %>%
summarize(onset_quantile_median = median(onset_quantile_eachyear),
onset_quantile_mean = mean(onset_quantile_eachyear)) %>%
ungroup()
quantile_map_summary <- ggplot(onset_oi_summarized) +
geom_sf(aes(fill = onset_quantile_median), color = NA) +
#scale_fill_viridis() +
scale_fill_gradient(low="black", high="blue") +
#scale_fill_brewer(palette="YlOrRd") +
ggtitle(paste("Median onset quantile, all years,", onset_type)) +
geom_polygon(data = MT_outline, aes(x = long, y = lat), color = "black",
alpha = 0, linetype = 1) +
theme_bw()
print(quantile_map_summary)
onset_types_toplot <-  c('Gabriel_onset', 'thres_10_persiann')
# c('Gabriel_onset', 'AA_2_persiann', 'AA_25_persiann', 'AA_3_persiann')
# c('Gabriel_onset', 'freq_5_persiann', 'freq_8_persiann', 'freq_10_persiann', 'freq_12_persiann')
# c('Gabriel_onset', 'monsoon_30_persiann', 'monsoon_40_persiann')
# c('Gabriel_onset', 'pentad_8_persiann', 'pentad_10_persiann', 'pentad_15_persiann')
# c('Gabriel_onset', 'rangethres_10_15_15_30_persiann', 'rangethres_10_15_15_40_persiann', 'rangethres_10_15_20_20_persiann', 'rangethres_10_15_20_30_persiann', 'rangethres_10_15_30_40_persiann')
# c('Gabriel_onset', 'thres_10_persiann')
onset_data_toplot <- all_onset_data[all_onset_data$onset_type %in% onset_types_toplot,]
# boxplot
ggplot(onset_data_toplot) +
geom_boxplot(aes(x=year_factor, y=onset, col = onset_type)) +
scale_color_brewer(palette="Blues") +
#scale_fill_viridis() +
ggtitle('Temporal patterns across all onset') +
labs(x = 'year') +
ylim(0, 130) +
theme_bw()
# group by year and onset_type, then calc median and stdev
onset_data_summarized_eachyear <- onset_data_toplot %>% group_by(year_factor, onset_type) %>%
summarize(onset_median = median(onset),
onset_stdev = sd(onset)) %>%
ungroup()
onset_data_summarized_pooledyear <- onset_data_toplot %>% group_by(onset_type) %>%
summarize(onset_median = median(onset),
onset_stdev = sd(onset)) %>%
ungroup()
onset_data_summarized_pooledyear$year_factor <- "all"
onset_data_summarized <- rbind(onset_data_summarized_eachyear, onset_data_summarized_pooledyear)
library(imputeTS)
library(dplyr)
library(signal) # Savitsky-Golay fitting
install.packages('signal')
install.packages('zoo')
install.packages("zoo")
library(imputeTS)
install.packages('zoo', type = 'binary')
library(imputeTS)
library(dplyr)
library(signal) # Savitsky-Golay fitting
library(dplyr)
library(zoo)
library(ggplot2)
# need to read in, because regardless of whether use smooth or unsmoothed EVI for the other fitting methods, must start with nonsmoothed EVI here. doubly smoothed EVI (20, 20) is in the csv
ts <- read.csv('point3.csv')[,1:2]
colnames(ts) <- c('date', 'EVI_raw')
ts$date <- as.Date(ts$date, '%d-%h-%y')
ts$day <- as.numeric(ts$date - as.Date('2016-08-01')) # number of days
year <- 2017
year_start = year - 1
year_end = year
# plot raw EVI for looking at datq quality, DOY for rising and falling limbs
plot(ts[ts$day >= 0 & ts$day <= 365,]$day, ts[ts$day >= 0 & ts$day <= 365,]$EVI_raw, main = 'raw EVI', xlab = 'DOY after Aug 1', ylab = 'EVI')
# function to smooth by certain number of days (not by index). numDays creates +/- window; so actual smoothing window is - numDays to + numDays
smooth_by_days <- function(numDays, t, values) {
smoothed <- numeric(length(values))
for (index in 1:length(values)) {
# get time window
day_center <- t[index]
day_start <- day_center - numDays
day_end <- day_center + numDays
# get indices corresponding to time window
indices <- which(t >= day_start & t <= day_end)
# get average of values over the indices and save
mean_value <- mean(values[indices], na.rm = TRUE)
smoothed[index] <- mean_value
}
return(smoothed)
}
# calculate first derivative of smoothed EVI (forward 1st order difference)
calc_1st_deriv <- function(values) {
derivatives <- numeric(length(values))
for (index in 1:(length(values)-1)) {
derivative <- values[index + 1] - values[index]
derivatives[index] <- derivative
}
derivatives[length(values)] <- derivatives[length(values) - 1] # to make sure the 1st derivative length still matches total time
return(derivatives)
}
harmonic_nls_simple_algorithm <- function(ts, window_EVI_1, window_EVI_2, numMissingPts, risingLimb_start_DOY, risingLimb_end_DOY, fallingLimb_start_DOY, fallingLimb_end_DOY, fittingStartDay, fittingEndDay) {
# cut EVI to only Aug 1, 2016 to April 1, 2017, then take out random points
targeted_start_day <- as.numeric(as.Date('2016-08-01') - as.Date('2016-08-01'))
targeted_end_day <- as.numeric(as.Date('2017-04-01') - as.Date('2016-08-01'))
targeted_days <- which(ts$day >= targeted_start_day & ts$day <= targeted_end_day) # index of days to target
ts <- ts[targeted_days,]
# cut out randomly selected missing dates; first make sure the points are sampled from days that have data
missing_days <- sample(unique(ts[complete.cases(ts),]$day), numMissingPts, replace = F) # days to take out
ts$EVI_raw[ts$day %in% missing_days] <- NA # replace the randomly selected dates' EVI_raw with 'NA'
# cut EVI to only fittingStartDay and fittingEndDay
fitting_start_day <- as.numeric(as.Date(fittingStartDay) - as.Date('2016-08-01'))
fitting_end_day <- as.numeric(as.Date(fittingEndDay) - as.Date('2016-08-01'))
fitting_days <- which(ts$day >= fitting_start_day & ts$day <= fitting_end_day) # index of days to fitting
ts <- ts[fitting_days,]
EVI_raw <- ts$EVI_raw
t <- ts$day
# smooth EVI series by 20 days, then 20 days
EVI_smoothed1 <- smooth_by_days(window_EVI_1, t, EVI_raw)
EVI_smoothed2 <- smooth_by_days(window_EVI_2, t, EVI_smoothed1)
# clean up EVI_smoothed2. for every date, replace with mean of the 2 EVI points and linearly interpolate at NaN times
ts_R <- data.frame(t, EVI_smoothed2)
ts_R <- ts_R %>% group_by(t) %>%
summarise_at(vars(EVI_smoothed2), mean)
# from here, the data won't be doubled on each date!
EVI_smoothed2 <- na.approx(ts_R$EVI_smoothed2, na.rm = FALSE)
# there may still be NA's at tails of EVI_smoothed2. replace these with the nearest non-NA or the average if have multiple NA's
NA_location <- which(is.na(EVI_smoothed2))
if (length(NA_location) == 1) { # if there is an NA in EVI_smoothed2, replace it with nearest value
if (NA_location == 1) {EVI_smoothed2[1] <- EVI_smoothed2[2]}
else if ( NA_location == length(EVI_smoothed2)) {EVI_smoothed2[length(EVI_smoothed2)] <- EVI_smoothed2[length(EVI_smoothed2) - 1]}
}
if (length(NA_location) > 1) {
EVI_smoothed2[which(is.na(EVI_smoothed2))] <- mean(EVI_smoothed2, na.rm = TRUE)
}
ts_R$EVI_smoothed2 <- EVI_smoothed2
# harmonic fitting
t_fitting <- ts_R$t/(2*3.14)
# Model that estimates w. if smoothing fails, use try to return NA's for midSeason and quarterPeriod_comparable and exit function
model_nls1 <- try(nls(EVI_smoothed2~c1 + c2*t_fitting + c3*(t_fitting^2) + c4*sin(w*t_fitting) + c5*cos(w*t_fitting),
start = list(c1 = -1, c2 = 0.1, c3 = 0.1,
c4 = 0.1, c5 = 0.1, w = 0.3
),
control = nls.control(maxiter = 2000, minFactor = 1e-7))
)
if(is(model_nls1, "try-error")) {
results <- c(NA, NA)
names(results) <- c('midSeason', 'quarterPd_comparable')
return(results)
}
# retreive omega from estimated coefficients
omega <- summary(model_nls1)$parameters[6]
#print('omega')
#print(omega)
period <- 365/(omega*2*3.14) # days
quarter_period_comparable <- period/4 # days
# find phenological dates, first make sure the dates found are for first crop, first or second half
firstHalf_dates <- which(ts_R$t >= risingLimb_start_DOY & ts_R$t <= risingLimb_end_DOY) # rising limb of first crop
t_firstHalf <- ts_R$t[firstHalf_dates]
fittedEVI_firstHalf <- fitted(model_nls1)[firstHalf_dates]
secondHalf_dates <- which(ts_R$t >= fallingLimb_start_DOY & ts_R$t <= fallingLimb_end_DOY) # falling limb of first crop
t_secondHalf <- ts_R$t[secondHalf_dates]
fittedEVI_secondHalf <- fitted(model_nls1)[secondHalf_dates]
firstCrop_dates <- which(ts_R$t >= risingLimb_start_DOY & ts_R$t <= fallingLimb_end_DOY) # first crop
t_firstCrop <- ts_R$t[firstCrop_dates]
fittedEVI_firstCrop <- fitted(model_nls1)[firstCrop_dates]
# date and value of max EVI
maxEVI <- max(fittedEVI_firstCrop, na.rm = TRUE)
minEVI_left <- min(fittedEVI_firstHalf, na.rm = TRUE)
minEVI_right <- min(fittedEVI_secondHalf, na.rm = TRUE)
date_min <- approx(x = fittedEVI_firstCrop, y = t_firstCrop, xout = minEVI_left)$y
date_max <- approx(x = fittedEVI_firstCrop, y = t_firstCrop, xout = maxEVI)$y
# find phenological dates
# rightOfPeakEVI <- 0.9*(maxEVI - minEVI_left) + minEVI_left
# leftOfPeakEVI <- 0.9*(maxEVI - minEVI_right) + minEVI_right
#
# date_rightOfPeak <- approx(x = fittedEVI_secondHalf, y = t_secondHalf, xout = rightOfPeakEVI)$y
# date_leftOfPeak <- approx(x = fittedEVI_firstHalf, y = t_firstHalf, xout = leftOfPeakEVI)$y
#
# if (is.na(date_rightOfPeak)) {date_rightOfPeak <- mean(risingLimb_start_DOY, fallingLimb_end_DOY)}
# if (is.na(date_leftOfPeak)) {date_leftOfPeak <- mean(risingLimb_start_DOY, fallingLimb_end_DOY)}
#
# midSeason <- mean(c(date_rightOfPeak, date_leftOfPeak))
results <- c(date_max, quarter_period_comparable)
names(results) <- c('midSeason', 'quarterPd_comparable')
# plot(ts_R$t, EVI_smoothed2, main = "1st order harmonic nls", xlab = "Days after August 1", ylab = "EVI", xlim = c(0, 350))
# lines(ts_R$t, EVI_smoothed2, col = "black")
# lines(ts_R$t, fitted(model_nls1), col = "red")
# abline(v = date_min, col = "blue")
# abline(v = date_max - quarter_period_comparable, col = "darkgreen")
# abline(v = date_max, col = "blue")
return(results)
}
# takes elimPoints, the vector of number of points to eliminate, and numRuns, the number of runs to do per number of points eliminated
run_harmonic_nls_simple_algorithm <- function(ts, numRuns, elimPoints, window_EVI_1, window_EVI_2, risingLimb_start_DOY, risingLimb_end_DOY, fallingLimb_start_DOY, fallingLimb_end_DOY, fittingStartDay, fittingEndDay) {
# to hold results
peak_results <- matrix(nrow = numRuns, ncol = length(elimPoints))
quarterPd_results <- matrix(nrow = numRuns, ncol = length(elimPoints))
# run with appropriate number of missing points
for (numMissingPts_index in 1:length(elimPoints)) {
for (runIndex in 1:numRuns) {
numMissingPts <- elimPoints[numMissingPts_index]
result <- harmonic_nls_simple_algorithm(ts, window_EVI_1, window_EVI_2, numMissingPts, risingLimb_start_DOY, risingLimb_end_DOY, fallingLimb_start_DOY, fallingLimb_end_DOY, fittingStartDay, fittingEndDay)
peak_results[runIndex, numMissingPts_index] <- result['midSeason']
quarterPd_results[runIndex, numMissingPts_index] <- result['quarterPd_comparable']
}
}
peak_means <- colMeans(peak_results, na.rm = TRUE)
peak_sd <- apply(peak_results, 2, sd, na.rm = TRUE)
quarterPd_means <- colMeans(quarterPd_results, na.rm = TRUE)
quarterPd_sd <- apply(quarterPd_results, 2, sd, na.rm = TRUE)
# combine stats into named matrix
results <- matrix(c(peak_means, peak_sd, quarterPd_means, quarterPd_sd), ncol = length(elimPoints), byrow = TRUE)
rownames(results) <- c('peak_mean', 'peak_sd', 'quarterPd_mean', 'quarterPd_sd')
colnames(results) <- elimPoints
return(results)
}
#print(run_harmonic_nls_simple_algorithm(ts, 10, 0, 10, 20, 40, 140, 130, 240, '2016-10-01', '2017-03-01'))
# change the smoothing, and plot how results change as data degrades
# smoothing_names is the name to assign to each combo of smoothing windows
# NOTE, the smoothing options are pairwise, NOT window_EVI_1_vector x window_EVI_2_vector
test_smoothing_harmonic_nls_simple <- function(ts, numRuns, elimPoints, window_EVI_1_vector, window_EVI_2_vector, smoothing_names, risingLimb_start_DOY, risingLimb_end_DOY, fallingLimb_start_DOY, fallingLimb_end_DOY, fittingStartDay, fittingEndDay) {
# add to this as get peak and quarter period results for each smoothing type
# jitter_amount is for plotting points that aren't completely overlaid on each other
plot_df <- data.frame(smoothing_name = character(0), eliminated_points = numeric(0), peak_mean = numeric(0), peak_sd = numeric(0),
quarterPd_mean = numeric(0), quarterPd_sd = numeric(0), jitter_amount = numeric(0))
for (smoothingIndex in 1:length(smoothing_names)) {
# get the smoothing parameters
window_EVI_1 <- window_EVI_1_vector[smoothingIndex]
window_EVI_2 <- window_EVI_2_vector[smoothingIndex]
smoothing_name <- smoothing_names[smoothingIndex]
# run algorithm the appropriate number of times
results <- run_harmonic_nls_simple_algorithm(ts, numRuns, elimPoints, window_EVI_1, window_EVI_2, risingLimb_start_DOY, risingLimb_end_DOY, fallingLimb_start_DOY, fallingLimb_end_DOY, fittingStartDay, fittingEndDay)
plot_df <- rbind(plot_df, data.frame(smoothing_name = rep(smoothing_name, length(elimPoints)), eliminated_points = elimPoints, peak_mean = results['peak_mean',], peak_sd = results['peak_sd',], quarterPd_mean = results['quarterPd_mean',], quarterPd_sd = results['quarterPd_sd',], jitter_amount = rep(smoothingIndex/10, length(elimPoints))))
# transpose results before cleaning and plotting
results <- as.data.frame(t(results))
results_toKeep <- complete.cases(results)
results <- results[results_toKeep,]
# plot the results
plot(elimPoints[results_toKeep], results$peak_mean, main = paste0('mean peak for ', smoothing_name), ylab = 'peak [day]', xlab = 'num points eliminated',
ylim = c(min(results$peak_mean-results$peak_sd), max(results$peak_mean+results$peak_sd)))
arrows(elimPoints[results_toKeep], results$peak_mean-results$peak_sd, elimPoints[results_toKeep], results$peak_mean+results$peak_sd, length=0.05, angle=90, code=3)
plot(elimPoints[results_toKeep], results$quarterPd_mean, main = paste0('mean quarterPd for ', smoothing_name), ylab = 'quarterPd [day]', xlab = 'num points eliminated',
ylim = c(min(results$quarterPd_mean-results$quarterPd_sd), max(results$quarterPd_mean+results$quarterPd_sd)))
arrows(elimPoints[results_toKeep], results$quarterPd_mean-results$quarterPd_sd, elimPoints[results_toKeep], results$quarterPd_mean+results$quarterPd_sd, length=0.05, angle=90, code=3)
}
plot_df <- plot_df[complete.cases(plot_df),]
plot_df$eliminated_points_toPlot <- plot_df$eliminated_points + plot_df$jitter_amount
plot_df <- plot_df %>% group_by(smoothing_name)
# PEAK PLOT --------------------------------------------------------------------------------------------------------------------------
peak_summary_plot <- ggplot(plot_df) +
geom_line(data = plot_df, aes(eliminated_points_toPlot, peak_mean, color = smoothing_name), size = 2) +
geom_point(data = plot_df, aes(eliminated_points_toPlot, peak_mean, color = smoothing_name)) +
#geom_errorbar(data = plot_df, aes(eliminated_points_toPlot, peak_mean, ymin = peak_mean - peak_sd, ymax = peak_mean + peak_sd, color = smoothing_name), width = 0.4) +
geom_ribbon(aes(x = eliminated_points_toPlot, ymin=peak_mean - peak_sd,
ymax=peak_mean + peak_sd, color = smoothing_name), alpha=0.2) +
scale_x_continuous(breaks=c(0,2,4,6,8,10)) +
ylim(110, 145) +
scale_color_manual(values = c('black', 'orange')) +
theme_bw() +
theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
panel.background = element_blank(), axis.line = element_line(colour = "black"),
axis.title.x=element_blank(), axis.text.x=element_blank(), axis.ticks.x=element_blank(),
axis.title.y=element_blank(), axis.text.y=element_blank(), axis.ticks.y=element_blank(),
text = element_text(size=15)) +
ggtitle('Peak, harmonic nls') +
xlab('Number of eliminated points') +
ylab('Peak DOY since Aug 1')
# QUARTER PERIOD PLOT ----------------------------------------------------------------------------------------------------------------
quarterPd_summary_plot <- ggplot(plot_df) +
geom_line(data = plot_df, aes(eliminated_points_toPlot, quarterPd_mean, color = smoothing_name), size = 2) +
geom_point(data = plot_df, aes(eliminated_points_toPlot, quarterPd_mean, color = smoothing_name)) +
#geom_errorbar(data = plot_df, aes(eliminated_points_toPlot, quarterPd_mean, ymin = quarterPd_mean - quarterPd_sd, ymax = quarterPd_mean + quarterPd_sd, color = smoothing_name), width = 0.4) +
geom_ribbon(aes(x = eliminated_points_toPlot, ymin=quarterPd_mean - quarterPd_sd,
ymax=quarterPd_mean + quarterPd_sd, color = smoothing_name), alpha=0.2) +
scale_x_continuous(breaks=c(0,2,4,6,8,10)) +
ylim(25, 55) +
scale_color_manual(values = c('black', 'orange')) +
theme_bw() +
theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
panel.background = element_blank(), axis.line = element_line(colour = "black"),
axis.title.y=element_blank(), axis.text.y=element_blank(), axis.ticks.y=element_blank(),
text = element_text(size=15)) +
ggtitle('Quarter period, harmonic nls') +
xlab('Number of eliminated points') +
ylab('Quarter period [days]')
print(peak_summary_plot)
print(quarterPd_summary_plot)
}
print(peak_summary_plot)
print(quarterPd_summary_plot)
# ts, window_EVI_1, window_EVI_2, numMissingPts, risingLimb_start_DOY, risingLimb_end_DOY, fallingLimb_start_DOY, fallingLimb_end_DOY, fittingStartDay, fittingEndDay
print(harmonic_nls_simple_algorithm(ts, 0, 0, 0, 10, 120, 110, 200, '2016-09-01', '2017-02-01'))
shiny::runApp('D:/R-code2/Webpage/cropTiming2')
install.packages('leafpop')
install.packages('spatialEco')
runApp('D:/R-code2/Webpage/cropTiming2')
